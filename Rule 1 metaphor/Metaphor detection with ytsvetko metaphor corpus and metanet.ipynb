{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at https://github.com/ytsvetko/metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##all the imports...\n",
    "#%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import pprint\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from nltk.collocations import *\n",
    "import string, random\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import bigrams\n",
    "from nltk import collocations\n",
    "from nltk import trigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by loading the .csv files and setting the columns.\n",
    "### For these .csv files, downloaded from https://github.com/ytsvetko/metaphor inputs and saved as .csv with the first row added as \"sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry welt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bald assertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bare outline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>black humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blind alley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample\n",
       "0      angry welt\n",
       "1  bald assertion\n",
       "2    bare outline\n",
       "3     black humor\n",
       "4     blind alley"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anmet = pd.read_csv(\"an_mets.csv\", low_memory=False)\n",
    "df_anmet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anmet = pd.read_csv(\"an_mets.csv\", low_memory=False)\n",
    "df_anmet['metaphor'] = 1\n",
    "df_anmet['an'] = 1\n",
    "df_anmet['svo'] = 0\n",
    "df_anmet['metanet'] = 0\n",
    "df_anmet.head()\n",
    "len(df_anmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annonmet = pd.read_csv(\"an_nonmets.csv\", low_memory=False)\n",
    "df_annonmet['metaphor'] = 0\n",
    "df_annonmet['an'] = 1\n",
    "df_annonmet['svo'] = 0\n",
    "df_annonmet['metanet'] = 0\n",
    "df_annonmet.head()\n",
    "len(df_annonmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svomet = pd.read_csv(\"svo_mets.csv\", low_memory=False)\n",
    "df_svomet['metaphor'] = 1\n",
    "df_svomet['an'] = 0\n",
    "df_svomet['svo'] = 1\n",
    "df_svomet['metanet'] = 0\n",
    "df_svomet.head()\n",
    "len(df_svomet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svononmet = pd.read_csv(\"svo_nonmets.csv\", low_memory=False)\n",
    "df_svononmet['metaphor'] = 0\n",
    "df_svononmet['an'] = 0\n",
    "df_svononmet['svo'] = 1\n",
    "df_svononmet['metanet'] = 0\n",
    "df_svononmet.head()\n",
    "len(df_svononmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>metaphor</th>\n",
       "      <th>an</th>\n",
       "      <th>svo</th>\n",
       "      <th>metanet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ability to evaluate government is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ability to evaluate is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability to know is ability to see</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abusive political leaders are physical bullies</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accepting is swallowing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sample  metaphor  an  svo  \\\n",
       "0  ability to evaluate government is ability to see         1   0    0   \n",
       "1             ability to evaluate is ability to see         1   0    0   \n",
       "2                 ability to know is ability to see         1   0    0   \n",
       "3    abusive political leaders are physical bullies         1   0    0   \n",
       "4                           accepting is swallowing         1   0    0   \n",
       "\n",
       "   metanet  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metanet = pd.read_csv(\"metanet.csv\", low_memory=False)\n",
    "df_metanet['metaphor'] = 1\n",
    "df_metanet['an'] = 0\n",
    "df_metanet['svo'] = 0\n",
    "df_metanet['metanet'] = 1\n",
    "df_metanet.head()\n",
    "# len(df_metanet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>metaphor</th>\n",
       "      <th>an</th>\n",
       "      <th>svo</th>\n",
       "      <th>metanet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>conversation turn subject</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>resumption bring relief</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>economy move direction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>service meet expectation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>material live dream</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>unemployment stand *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>action talk *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>statement sit *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>income fall *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>silence speak volume</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>car decide *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>battery die *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>accident wait *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Texan break record</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>temperature break *number</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>insurance cover care</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>state cut spending</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Hawaii kill  proposal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>electronics drive innovation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>teenager wear attitude</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>*pronoun catch flight</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>envy eat *none</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>data point pain</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>advertiser pull ad</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>*pronoun close deal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>police close investigation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>bicycle suffer damage</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Tori throw tantrum</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>fortune smile *pronoun</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>excitement fill street</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>analysis of social problems is diagnosis of af...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>analyzing is dissecting</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>anger is fire</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>anger is heat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>anger is insanity</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>anger is pressure in a container</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>anger is the heat of fluid in a container</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>argument is physical combat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>argument is war</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>arithmetic is object construction</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>assessing is measuring</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>assistance is support</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>attaining control is gaining a possession</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>attributes are entities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>attributes are possessions</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>attributes of government are entities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>augmenting economic assets is creating objects</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>bad is stinky</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>becoming impoverished is moving downwards</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>being alive is being physically at this location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>being good is being upright</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>being immoral is being low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>being impoverished is being at a low location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>being impoverished is being in a bounded region</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>being in a high social class is being high on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>being in a low social class is being low on a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>being in a middle class is being in the middle...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>being in a state is being at a point on a line...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>being poised to know is being positioned to se...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>being wealthy is being at a high location</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sample  metaphor  an  svo  \\\n",
       "200                          conversation turn subject         1   0    1   \n",
       "201                            resumption bring relief         1   0    1   \n",
       "202                             economy move direction         1   0    1   \n",
       "203                           service meet expectation         1   0    1   \n",
       "204                                material live dream         1   0    1   \n",
       "205                           unemployment stand *none         1   0    1   \n",
       "206                                  action talk *none         1   0    1   \n",
       "207                                statement sit *none         1   0    1   \n",
       "208                                  income fall *none         1   0    1   \n",
       "209                               silence speak volume         1   0    1   \n",
       "210                                   car decide *none         1   0    1   \n",
       "211                                  battery die *none         1   0    1   \n",
       "212                                accident wait *none         1   0    1   \n",
       "213                                 Texan break record         1   0    1   \n",
       "214                          temperature break *number         1   0    1   \n",
       "215                               insurance cover care         1   0    1   \n",
       "216                                 state cut spending         1   0    1   \n",
       "217                              Hawaii kill  proposal         1   0    1   \n",
       "218                       electronics drive innovation         1   0    1   \n",
       "219                             teenager wear attitude         1   0    1   \n",
       "220                              *pronoun catch flight         1   0    1   \n",
       "221                                     envy eat *none         1   0    1   \n",
       "222                                    data point pain         1   0    1   \n",
       "223                                 advertiser pull ad         1   0    1   \n",
       "224                                *pronoun close deal         1   0    1   \n",
       "225                         police close investigation         1   0    1   \n",
       "226                              bicycle suffer damage         1   0    1   \n",
       "227                                 Tori throw tantrum         1   0    1   \n",
       "228                             fortune smile *pronoun         1   0    1   \n",
       "229                             excitement fill street         1   0    1   \n",
       "..                                                 ...       ...  ..  ...   \n",
       "470  analysis of social problems is diagnosis of af...         1   0    0   \n",
       "471                            analyzing is dissecting         1   0    0   \n",
       "472                                      anger is fire         1   0    0   \n",
       "473                                      anger is heat         1   0    0   \n",
       "474                                  anger is insanity         1   0    0   \n",
       "475                   anger is pressure in a container         1   0    0   \n",
       "476          anger is the heat of fluid in a container         1   0    0   \n",
       "477                        argument is physical combat         1   0    0   \n",
       "478                                    argument is war         1   0    0   \n",
       "479                  arithmetic is object construction         1   0    0   \n",
       "480                             assessing is measuring         1   0    0   \n",
       "481                              assistance is support         1   0    0   \n",
       "482          attaining control is gaining a possession         1   0    0   \n",
       "483                            attributes are entities         1   0    0   \n",
       "484                         attributes are possessions         1   0    0   \n",
       "485              attributes of government are entities         1   0    0   \n",
       "486     augmenting economic assets is creating objects         1   0    0   \n",
       "487                                      bad is stinky         1   0    0   \n",
       "488          becoming impoverished is moving downwards         1   0    0   \n",
       "489   being alive is being physically at this location         1   0    0   \n",
       "490                        being good is being upright         1   0    0   \n",
       "491                         being immoral is being low         1   0    0   \n",
       "492      being impoverished is being at a low location         1   0    0   \n",
       "493    being impoverished is being in a bounded region         1   0    0   \n",
       "494  being in a high social class is being high on ...         1   0    0   \n",
       "495  being in a low social class is being low on a ...         1   0    0   \n",
       "496  being in a middle class is being in the middle...         1   0    0   \n",
       "497  being in a state is being at a point on a line...         1   0    0   \n",
       "498  being poised to know is being positioned to se...         1   0    0   \n",
       "499          being wealthy is being at a high location         1   0    0   \n",
       "\n",
       "     metanet  \n",
       "200        0  \n",
       "201        0  \n",
       "202        0  \n",
       "203        0  \n",
       "204        0  \n",
       "205        0  \n",
       "206        0  \n",
       "207        0  \n",
       "208        0  \n",
       "209        0  \n",
       "210        0  \n",
       "211        0  \n",
       "212        0  \n",
       "213        0  \n",
       "214        0  \n",
       "215        0  \n",
       "216        0  \n",
       "217        0  \n",
       "218        0  \n",
       "219        0  \n",
       "220        0  \n",
       "221        0  \n",
       "222        0  \n",
       "223        0  \n",
       "224        0  \n",
       "225        0  \n",
       "226        0  \n",
       "227        0  \n",
       "228        0  \n",
       "229        0  \n",
       "..       ...  \n",
       "470        1  \n",
       "471        1  \n",
       "472        1  \n",
       "473        1  \n",
       "474        1  \n",
       "475        1  \n",
       "476        1  \n",
       "477        1  \n",
       "478        1  \n",
       "479        1  \n",
       "480        1  \n",
       "481        1  \n",
       "482        1  \n",
       "483        1  \n",
       "484        1  \n",
       "485        1  \n",
       "486        1  \n",
       "487        1  \n",
       "488        1  \n",
       "489        1  \n",
       "490        1  \n",
       "491        1  \n",
       "492        1  \n",
       "493        1  \n",
       "494        1  \n",
       "495        1  \n",
       "496        1  \n",
       "497        1  \n",
       "498        1  \n",
       "499        1  \n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine into one df\n",
    "frames = [df_anmet, df_annonmet, df_svomet, df_svononmet, df_metanet]\n",
    "df_combo = pd.concat(frames)\n",
    "df_combo.reset_index(drop=True, inplace=True)\n",
    "df_combo.shape\n",
    "# df_combo.replace(to_replace='none', value=\"\")\n",
    "# #         tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# #         word_list = tokenizer.tokenize(line)\n",
    "# #         filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "# df_combo.replace(to_replace='is', value=\"\")\n",
    "stop = ['*none', '.', 'is']\n",
    "df_combo['sample'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df_combo[200:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break down into training, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_index = np.random.permutation(df_combo.index)\n",
    "df_combo.ix[random_index, ['sample', 'metaphor', 'an', 'svo', 'metanet']]\n",
    "df_shuffled = df_combo.ix[random_index, ['sample', 'metaphor', 'an', 'svo', 'metanet']]\n",
    "df_shuffled.reset_index(drop=True, inplace=True)\n",
    "len(df_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1606\n",
      "Columns: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = df_shuffled.shape\n",
    "print(\"Rows:\", rows)\n",
    "print(\"Columns:\", columns)\n",
    "#train_size = round(rows*.6)\n",
    "train_size = round(rows*.9)\n",
    "#dev_size   = round(rows*.2)\n",
    "dev_size   = round(rows*.1)\n",
    "df_train = df_shuffled.loc[:train_size]\n",
    "df_train.shape\n",
    "df_dev = df_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
    "df_dev.shape\n",
    "df_test = df_shuffled.loc[dev_size+train_size:].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "### Use count vecotrizer with df = 3 for unigram and bigrams to predict dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(set(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vec = CountVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b', analyzer=u'word', min_df=1, vocabulary=vocab)\n",
    "# df_train = df_train.fillna(\"\")\n",
    "# df_dev = df_dev.fillna(\"\")\n",
    "# df_test = df_test.fillna(\"\")\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(2, 3), token_pattern=r'\\b\\w+\\b', analyzer=u'char', min_df=1)\n",
    "df_train = df_train.fillna(\"\")\n",
    "df_dev = df_dev.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_train_feature_sparse = vec.fit_transform(df_train['sample'])\n",
    "arr_train_feature_sparse\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "feature_labels = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_dev_feature_sparse = vec.transform(df_dev[\"sample\"])\n",
    "arr_dev_feature = arr_dev_feature_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91925465838509313"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor']) #defining features (from reviews) and passing in Category label\n",
    "logreg_predictions = logreg_model.predict(arr_dev_feature)\n",
    "accuracy_score(df_dev['metaphor'], logreg_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>56.150914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>53.809020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>51.863619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>50.957381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>49.962174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ti</th>\n",
       "      <td>49.516231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>49.403087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>44.935289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>44.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>re</th>\n",
       "      <td>41.980039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        counts\n",
       "s    56.150914\n",
       "in   53.809020\n",
       " i   51.863619\n",
       "is   50.957381\n",
       " a   49.962174\n",
       "ti   49.516231\n",
       "on   49.403087\n",
       "is   44.935289\n",
       " is  44.639800\n",
       "re   41.980039"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sum = arr_train_feature.sum(axis=0)   #sum the counts of each feature\n",
    "\n",
    "df_feature_sum = pd.DataFrame({'counts': feature_sum})\n",
    "df_feature_sum.index = vec.get_feature_names()\n",
    "df_feature_sum.sort('counts', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make a file of sample sentences to practice on\n",
    "## loop through sentence and .split()\n",
    "## use nltk bigrams\n",
    "##\n",
    "## put into pandas as df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jamal', 'pig', 'dinner']\n"
     ]
    }
   ],
   "source": [
    "word_list = 'Jamal was a pig at dinner'.split()\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "filtered_words\n",
    "testmeta = list(trigrams(filtered_words))\n",
    "testmeta\n",
    "testmeta1 = [' '.join(x) for x in testmeta]\n",
    "testmeta1\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test.columns = ['sample']\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "logreg_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jamal pig dinner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "0  Jamal pig dinner"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "#         word_list = line.split()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        word_list = tokenizer.tokenize(line)\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "#         testmeta1.append(' '.join(filtered_words))\n",
    "        testmeta1.append(' '.join(x) for x in bigrams(filtered_words))\n",
    "#         testmeta1.append(' '.join(x) for x in filtered_words)\n",
    "# print(testmeta1)\n",
    "\n",
    "df_test =pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test\n",
    "# df_test.columns = ['sample']\n",
    "# df_test = pd.DataFrame(testmeta1)\n",
    "# df_test\n",
    "df_test.columns = ['sample0', 'sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6']\n",
    "# df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>sample1</th>\n",
       "      <th>sample2</th>\n",
       "      <th>sample3</th>\n",
       "      <th>sample4</th>\n",
       "      <th>sample5</th>\n",
       "      <th>sample6</th>\n",
       "      <th>metaphor0</th>\n",
       "      <th>metaphor1</th>\n",
       "      <th>metaphor2</th>\n",
       "      <th>metaphor3</th>\n",
       "      <th>metaphor4</th>\n",
       "      <th>metaphor5</th>\n",
       "      <th>metaphor6</th>\n",
       "      <th>sum_metaphor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The snow</td>\n",
       "      <td>snow white</td>\n",
       "      <td>white blanket</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The hospital</td>\n",
       "      <td>hospital refrigerator</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The classroom</td>\n",
       "      <td>classroom zoo</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>America melting</td>\n",
       "      <td>melting pot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Her lovely</td>\n",
       "      <td>lovely voice</td>\n",
       "      <td>voice music</td>\n",
       "      <td>music ears</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Life roller</td>\n",
       "      <td>roller coaster</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The alligator</td>\n",
       "      <td>alligator teeth</td>\n",
       "      <td>teeth white</td>\n",
       "      <td>white daggers</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Their home</td>\n",
       "      <td>home prison</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The slide</td>\n",
       "      <td>slide playground</td>\n",
       "      <td>playground hot</td>\n",
       "      <td>hot stove</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>His heart</td>\n",
       "      <td>heart cold</td>\n",
       "      <td>cold iron</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>She peacock</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>He shining</td>\n",
       "      <td>shining star</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Time money</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>My teacher</td>\n",
       "      <td>teacher dragon</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tom eyes</td>\n",
       "      <td>eyes ice</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The detective</td>\n",
       "      <td>detective face</td>\n",
       "      <td>face wood</td>\n",
       "      <td>wood listened</td>\n",
       "      <td>listened story</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>She feels</td>\n",
       "      <td>feels life</td>\n",
       "      <td>life fashion</td>\n",
       "      <td>fashion show</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The world</td>\n",
       "      <td>world stage</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My kid</td>\n",
       "      <td>kid room</td>\n",
       "      <td>room disaster</td>\n",
       "      <td>disaster area</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The children</td>\n",
       "      <td>children flowers</td>\n",
       "      <td>flowers grown</td>\n",
       "      <td>grown concrete</td>\n",
       "      <td>concrete gardens</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kisses flowers</td>\n",
       "      <td>flowers affection</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>His words</td>\n",
       "      <td>words cotton</td>\n",
       "      <td>cotton candy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mary eyes</td>\n",
       "      <td>eyes fireflies</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>John suggestion</td>\n",
       "      <td>suggestion Band</td>\n",
       "      <td>Band Aid</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The cast</td>\n",
       "      <td>cast broken</td>\n",
       "      <td>broken leg</td>\n",
       "      <td>leg plaster</td>\n",
       "      <td>plaster shackle</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Jane ambitions</td>\n",
       "      <td>ambitions house</td>\n",
       "      <td>house cards</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Her long</td>\n",
       "      <td>long hair</td>\n",
       "      <td>hair flowing</td>\n",
       "      <td>flowing golden</td>\n",
       "      <td>golden river</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The computers</td>\n",
       "      <td>computers school</td>\n",
       "      <td>school old</td>\n",
       "      <td>old dinosaurs</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Laughter music</td>\n",
       "      <td>music soul</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>He night</td>\n",
       "      <td>night owl</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Your brain</td>\n",
       "      <td>brain computer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Jamal pig</td>\n",
       "      <td>pig dinner</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>You sunshine</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The car</td>\n",
       "      <td>car furnace</td>\n",
       "      <td>furnace sun</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Thank much</td>\n",
       "      <td>much You</td>\n",
       "      <td>You angel</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>That coach</td>\n",
       "      <td>coach ogre</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Ben temper</td>\n",
       "      <td>temper volcano</td>\n",
       "      <td>volcano ready</td>\n",
       "      <td>ready explode</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>The kids</td>\n",
       "      <td>kids monkeys</td>\n",
       "      <td>monkeys jungle</td>\n",
       "      <td>jungle gym</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>The sun</td>\n",
       "      <td>sun golden</td>\n",
       "      <td>golden ball</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>The clouds</td>\n",
       "      <td>clouds balls</td>\n",
       "      <td>balls cotton</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Sue room</td>\n",
       "      <td>room zoo</td>\n",
       "      <td>zoo fish</td>\n",
       "      <td>fish gerbil</td>\n",
       "      <td>gerbil parakeet</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>The park</td>\n",
       "      <td>park lake</td>\n",
       "      <td>lake rain</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>The lightning</td>\n",
       "      <td>lightning fireworks</td>\n",
       "      <td>fireworks sky</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Gary mule</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>That lawn</td>\n",
       "      <td>lawn green</td>\n",
       "      <td>green carpet</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>My dad</td>\n",
       "      <td>dad road</td>\n",
       "      <td>road hog</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>The stars</td>\n",
       "      <td>stars sparkling</td>\n",
       "      <td>sparkling diamonds</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Those two</td>\n",
       "      <td>two best</td>\n",
       "      <td>best friends</td>\n",
       "      <td>friends two</td>\n",
       "      <td>two peas</td>\n",
       "      <td>peas pod</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>He walking</td>\n",
       "      <td>walking dictionary</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Donations popular</td>\n",
       "      <td>popular charity</td>\n",
       "      <td>charity tsunami</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Necessity mother</td>\n",
       "      <td>mother invention</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>My big</td>\n",
       "      <td>big brother</td>\n",
       "      <td>brother couch</td>\n",
       "      <td>couch potato</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>The road</td>\n",
       "      <td>road ribbon</td>\n",
       "      <td>ribbon stretching</td>\n",
       "      <td>stretching across</td>\n",
       "      <td>across desert</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>The teenager</td>\n",
       "      <td>teenager stomach</td>\n",
       "      <td>stomach bottomless</td>\n",
       "      <td>bottomless pit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>The thunder</td>\n",
       "      <td>thunder mighty</td>\n",
       "      <td>mighty lion</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>I excited</td>\n",
       "      <td>excited My</td>\n",
       "      <td>My pulse</td>\n",
       "      <td>pulse race</td>\n",
       "      <td>race car</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>The moon</td>\n",
       "      <td>moon white</td>\n",
       "      <td>white balloon</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Toddlers rug</td>\n",
       "      <td>rug rats</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The stormy</td>\n",
       "      <td>stormy ocean</td>\n",
       "      <td>ocean raging</td>\n",
       "      <td>raging bull</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Her tears</td>\n",
       "      <td>tears river</td>\n",
       "      <td>river flowing</td>\n",
       "      <td>flowing cheeks</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              sample0                sample1             sample2  \\\n",
       "0            The snow             snow white       white blanket   \n",
       "1        The hospital  hospital refrigerator                       \n",
       "2       The classroom          classroom zoo                       \n",
       "3     America melting            melting pot                       \n",
       "4          Her lovely           lovely voice         voice music   \n",
       "5         Life roller         roller coaster                       \n",
       "6       The alligator        alligator teeth         teeth white   \n",
       "7          Their home            home prison                       \n",
       "8           The slide       slide playground      playground hot   \n",
       "9           His heart             heart cold           cold iron   \n",
       "10        She peacock                                              \n",
       "11         He shining           shining star                       \n",
       "12         Time money                                              \n",
       "13         My teacher         teacher dragon                       \n",
       "14           Tom eyes               eyes ice                       \n",
       "15      The detective         detective face           face wood   \n",
       "16          She feels             feels life        life fashion   \n",
       "17          The world            world stage                       \n",
       "18             My kid               kid room       room disaster   \n",
       "19       The children       children flowers       flowers grown   \n",
       "20     Kisses flowers      flowers affection                       \n",
       "21          His words           words cotton        cotton candy   \n",
       "22          Mary eyes         eyes fireflies                       \n",
       "23    John suggestion        suggestion Band            Band Aid   \n",
       "24           The cast            cast broken          broken leg   \n",
       "25     Jane ambitions        ambitions house         house cards   \n",
       "26           Her long              long hair        hair flowing   \n",
       "27      The computers       computers school          school old   \n",
       "28     Laughter music             music soul                       \n",
       "29           He night              night owl                       \n",
       "..                ...                    ...                 ...   \n",
       "41         Your brain         brain computer                       \n",
       "42          Jamal pig             pig dinner                       \n",
       "43       You sunshine                                              \n",
       "44            The car            car furnace         furnace sun   \n",
       "45         Thank much               much You           You angel   \n",
       "46         That coach             coach ogre                       \n",
       "47         Ben temper         temper volcano       volcano ready   \n",
       "48           The kids           kids monkeys      monkeys jungle   \n",
       "49            The sun             sun golden         golden ball   \n",
       "50         The clouds           clouds balls        balls cotton   \n",
       "51           Sue room               room zoo            zoo fish   \n",
       "52           The park              park lake           lake rain   \n",
       "53      The lightning    lightning fireworks       fireworks sky   \n",
       "54          Gary mule                                              \n",
       "55          That lawn             lawn green        green carpet   \n",
       "56             My dad               dad road            road hog   \n",
       "57          The stars        stars sparkling  sparkling diamonds   \n",
       "58          Those two               two best        best friends   \n",
       "59         He walking     walking dictionary                       \n",
       "60  Donations popular        popular charity     charity tsunami   \n",
       "61   Necessity mother       mother invention                       \n",
       "62             My big            big brother       brother couch   \n",
       "63           The road            road ribbon   ribbon stretching   \n",
       "64       The teenager       teenager stomach  stomach bottomless   \n",
       "65        The thunder         thunder mighty         mighty lion   \n",
       "66          I excited             excited My            My pulse   \n",
       "67           The moon             moon white       white balloon   \n",
       "68       Toddlers rug               rug rats                       \n",
       "69         The stormy           stormy ocean        ocean raging   \n",
       "70          Her tears            tears river       river flowing   \n",
       "\n",
       "              sample3           sample4   sample5 sample6  metaphor0  \\\n",
       "0                                                                  1   \n",
       "1                                                                  1   \n",
       "2                                                                  1   \n",
       "3                                                                  1   \n",
       "4          music ears                                              1   \n",
       "5                                                                  1   \n",
       "6       white daggers                                              1   \n",
       "7                                                                  1   \n",
       "8           hot stove                                              1   \n",
       "9                                                                  1   \n",
       "10                                                                 1   \n",
       "11                                                                 1   \n",
       "12                                                                 1   \n",
       "13                                                                 1   \n",
       "14                                                                 1   \n",
       "15      wood listened    listened story                            1   \n",
       "16       fashion show                                              1   \n",
       "17                                                                 1   \n",
       "18      disaster area                                              1   \n",
       "19     grown concrete  concrete gardens                            1   \n",
       "20                                                                 1   \n",
       "21                                                                 1   \n",
       "22                                                                 1   \n",
       "23                                                                 1   \n",
       "24        leg plaster   plaster shackle                            1   \n",
       "25                                                                 1   \n",
       "26     flowing golden      golden river                            1   \n",
       "27      old dinosaurs                                              1   \n",
       "28                                                                 1   \n",
       "29                                                                 1   \n",
       "..                ...               ...       ...     ...        ...   \n",
       "41                                                                 1   \n",
       "42                                                                 1   \n",
       "43                                                                 1   \n",
       "44                                                                 1   \n",
       "45                                                                 1   \n",
       "46                                                                 1   \n",
       "47      ready explode                                              1   \n",
       "48         jungle gym                                              1   \n",
       "49                                                                 1   \n",
       "50                                                                 1   \n",
       "51        fish gerbil   gerbil parakeet                            1   \n",
       "52                                                                 1   \n",
       "53                                                                 1   \n",
       "54                                                                 1   \n",
       "55                                                                 1   \n",
       "56                                                                 1   \n",
       "57                                                                 1   \n",
       "58        friends two          two peas  peas pod                  1   \n",
       "59                                                                 1   \n",
       "60                                                                 1   \n",
       "61                                                                 1   \n",
       "62       couch potato                                              1   \n",
       "63  stretching across     across desert                            0   \n",
       "64     bottomless pit                                              1   \n",
       "65                                                                 1   \n",
       "66         pulse race          race car                            1   \n",
       "67                                                                 1   \n",
       "68                                                                 1   \n",
       "69        raging bull                                              1   \n",
       "70     flowing cheeks                                              0   \n",
       "\n",
       "    metaphor1  metaphor2  metaphor3  metaphor4  metaphor5  metaphor6  \\\n",
       "0           1          1          1          1          1          1   \n",
       "1           1          1          1          1          1          1   \n",
       "2           1          1          1          1          1          1   \n",
       "3           1          1          1          1          1          1   \n",
       "4           1          1          1          1          1          1   \n",
       "5           1          1          1          1          1          1   \n",
       "6           1          1          1          1          1          1   \n",
       "7           1          1          1          1          1          1   \n",
       "8           1          1          1          1          1          1   \n",
       "9           1          1          1          1          1          1   \n",
       "10          1          1          1          1          1          1   \n",
       "11          1          1          1          1          1          1   \n",
       "12          1          1          1          1          1          1   \n",
       "13          1          1          1          1          1          1   \n",
       "14          1          1          1          1          1          1   \n",
       "15          1          1          1          1          1          1   \n",
       "16          1          1          1          1          1          1   \n",
       "17          1          1          1          1          1          1   \n",
       "18          1          1          1          1          1          1   \n",
       "19          1          1          1          1          1          1   \n",
       "20          1          1          1          1          1          1   \n",
       "21          1          0          1          1          1          1   \n",
       "22          1          1          1          1          1          1   \n",
       "23          1          1          1          1          1          1   \n",
       "24          1          1          1          0          1          1   \n",
       "25          1          1          1          1          1          1   \n",
       "26          1          1          1          1          1          1   \n",
       "27          1          1          1          1          1          1   \n",
       "28          1          1          1          1          1          1   \n",
       "29          1          1          1          1          1          1   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "41          1          1          1          1          1          1   \n",
       "42          1          1          1          1          1          1   \n",
       "43          1          1          1          1          1          1   \n",
       "44          1          1          1          1          1          1   \n",
       "45          1          1          1          1          1          1   \n",
       "46          1          1          1          1          1          1   \n",
       "47          1          1          1          1          1          1   \n",
       "48          1          1          1          1          1          1   \n",
       "49          1          1          1          1          1          1   \n",
       "50          1          1          1          1          1          1   \n",
       "51          1          1          1          1          1          1   \n",
       "52          1          1          1          1          1          1   \n",
       "53          1          1          1          1          1          1   \n",
       "54          1          1          1          1          1          1   \n",
       "55          1          1          1          1          1          1   \n",
       "56          0          0          1          1          1          1   \n",
       "57          1          1          1          1          1          1   \n",
       "58          1          1          1          1          1          1   \n",
       "59          1          1          1          1          1          1   \n",
       "60          1          1          1          1          1          1   \n",
       "61          1          1          1          1          1          1   \n",
       "62          0          1          1          1          1          1   \n",
       "63          0          1          1          1          1          1   \n",
       "64          1          1          1          1          1          1   \n",
       "65          1          1          1          1          1          1   \n",
       "66          1          1          1          1          1          1   \n",
       "67          1          1          1          1          1          1   \n",
       "68          1          1          1          1          1          1   \n",
       "69          1          1          1          1          1          1   \n",
       "70          1          1          1          1          1          1   \n",
       "\n",
       "    sum_metaphor  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "5              1  \n",
       "6              1  \n",
       "7              1  \n",
       "8              1  \n",
       "9              1  \n",
       "10             1  \n",
       "11             1  \n",
       "12             1  \n",
       "13             1  \n",
       "14             1  \n",
       "15             1  \n",
       "16             1  \n",
       "17             1  \n",
       "18             1  \n",
       "19             1  \n",
       "20             1  \n",
       "21             1  \n",
       "22             1  \n",
       "23             1  \n",
       "24             1  \n",
       "25             1  \n",
       "26             1  \n",
       "27             1  \n",
       "28             1  \n",
       "29             1  \n",
       "..           ...  \n",
       "41             1  \n",
       "42             1  \n",
       "43             1  \n",
       "44             1  \n",
       "45             1  \n",
       "46             1  \n",
       "47             1  \n",
       "48             1  \n",
       "49             1  \n",
       "50             1  \n",
       "51             1  \n",
       "52             1  \n",
       "53             1  \n",
       "54             1  \n",
       "55             1  \n",
       "56             1  \n",
       "57             1  \n",
       "58             1  \n",
       "59             1  \n",
       "60             1  \n",
       "61             1  \n",
       "62             1  \n",
       "63             1  \n",
       "64             1  \n",
       "65             1  \n",
       "66             1  \n",
       "67             1  \n",
       "68             1  \n",
       "69             1  \n",
       "70             1  \n",
       "\n",
       "[71 rows x 15 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample1'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor1'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample2'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor2'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample3'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor3'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample4'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor4'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample5'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor5'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample6'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor6'] = logreg_predictions\n",
    "\n",
    "df_test['sum_metaphor'] = df_test['metaphor0'] + df_test['metaphor1'] + df_test['metaphor2'] + df_test['metaphor3'] +df_test['metaphor4'] +df_test['metaphor5'] + df_test['metaphor6'] \n",
    "\n",
    "def count_metaphors():\n",
    "    if sum(df_test['sum_metaphor']) >= 1:\n",
    "        df_test['sum_metaphor'] = 1\n",
    "    else:\n",
    "        df_test['sum_metaphor'] = 0\n",
    "count_metaphors()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"an_nonmets.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta1.append(' '.join(x) for x in bigrams(filtered_words))\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "sum(df_test['metaphor0'])\n",
    "#len(df_test['metaphor0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>sample1</th>\n",
       "      <th>sample2</th>\n",
       "      <th>sample3</th>\n",
       "      <th>sample4</th>\n",
       "      <th>sample5</th>\n",
       "      <th>sample6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientists created “atlas</td>\n",
       "      <td>created “atlas brain”</td>\n",
       "      <td>“atlas brain” reveals</td>\n",
       "      <td>brain” reveals meanings</td>\n",
       "      <td>reveals meanings words</td>\n",
       "      <td>meanings words arranged</td>\n",
       "      <td>words arranged across</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like colourful quilt</td>\n",
       "      <td>colourful quilt laid</td>\n",
       "      <td>quilt laid cortex,</td>\n",
       "      <td>laid cortex, atlas</td>\n",
       "      <td>cortex, atlas displays</td>\n",
       "      <td>atlas displays rainbow</td>\n",
       "      <td>displays rainbow hues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Our goal build</td>\n",
       "      <td>goal build giant</td>\n",
       "      <td>build giant atlas</td>\n",
       "      <td>giant atlas shows</td>\n",
       "      <td>atlas shows one</td>\n",
       "      <td>shows one specific</td>\n",
       "      <td>one specific aspect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No single brain</td>\n",
       "      <td>single brain region</td>\n",
       "      <td>brain region holds</td>\n",
       "      <td>region holds one</td>\n",
       "      <td>holds one word</td>\n",
       "      <td>one word concept.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A single brain</td>\n",
       "      <td>single brain spot</td>\n",
       "      <td>brain spot associated</td>\n",
       "      <td>spot associated number</td>\n",
       "      <td>associated number related</td>\n",
       "      <td>number related words.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And single word</td>\n",
       "      <td>single word lights</td>\n",
       "      <td>word lights many</td>\n",
       "      <td>lights many different</td>\n",
       "      <td>many different brain</td>\n",
       "      <td>different brain spots.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Together make networks</td>\n",
       "      <td>make networks represent</td>\n",
       "      <td>networks represent meanings</td>\n",
       "      <td>represent meanings word</td>\n",
       "      <td>meanings word use:</td>\n",
       "      <td>word use: life</td>\n",
       "      <td>use: life love;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>All light networks.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Described “tour de</td>\n",
       "      <td>“tour de force”</td>\n",
       "      <td>de force” one</td>\n",
       "      <td>force” one researcher</td>\n",
       "      <td>one researcher involved</td>\n",
       "      <td>researcher involved study,</td>\n",
       "      <td>involved study, atlas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>With advances, technology</td>\n",
       "      <td>advances, technology could</td>\n",
       "      <td>technology could profound</td>\n",
       "      <td>could profound impact</td>\n",
       "      <td>profound impact medicine</td>\n",
       "      <td>impact medicine fields.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>“It possible approach</td>\n",
       "      <td>possible approach could</td>\n",
       "      <td>approach could used</td>\n",
       "      <td>could used decode</td>\n",
       "      <td>used decode information</td>\n",
       "      <td>decode information words</td>\n",
       "      <td>information words person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One potential use</td>\n",
       "      <td>potential use would</td>\n",
       "      <td>use would language</td>\n",
       "      <td>would language decoder</td>\n",
       "      <td>language decoder could</td>\n",
       "      <td>decoder could allow</td>\n",
       "      <td>could allow people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>To create atlas,</td>\n",
       "      <td>create atlas, scientists</td>\n",
       "      <td>atlas, scientists recorded</td>\n",
       "      <td>scientists recorded people’s</td>\n",
       "      <td>recorded people’s brain</td>\n",
       "      <td>people’s brain activity</td>\n",
       "      <td>brain activity listened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>They matched transcripts</td>\n",
       "      <td>matched transcripts stories</td>\n",
       "      <td>transcripts stories brain</td>\n",
       "      <td>stories brain activity</td>\n",
       "      <td>brain activity data</td>\n",
       "      <td>activity data show</td>\n",
       "      <td>data show groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Huth used stories</td>\n",
       "      <td>used stories The</td>\n",
       "      <td>stories The Moth</td>\n",
       "      <td>The Moth Radio</td>\n",
       "      <td>Moth Radio Hour</td>\n",
       "      <td>Radio Hour short</td>\n",
       "      <td>Hour short compelling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The enthralling stories,</td>\n",
       "      <td>enthralling stories, confident</td>\n",
       "      <td>stories, confident scientists</td>\n",
       "      <td>confident scientists could</td>\n",
       "      <td>scientists could people</td>\n",
       "      <td>could people scanned</td>\n",
       "      <td>people scanned focusing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Seven people listened</td>\n",
       "      <td>people listened two</td>\n",
       "      <td>listened two hours</td>\n",
       "      <td>two hours stories</td>\n",
       "      <td>hours stories each.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Per person, amounted</td>\n",
       "      <td>person, amounted hearing</td>\n",
       "      <td>amounted hearing roughly</td>\n",
       "      <td>hearing roughly 25,000</td>\n",
       "      <td>roughly 25,000 words-</td>\n",
       "      <td>25,000 words- 3,000</td>\n",
       "      <td>words- 3,000 different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The atlas shows</td>\n",
       "      <td>atlas shows words</td>\n",
       "      <td>shows words related</td>\n",
       "      <td>words related terms</td>\n",
       "      <td>related terms exercise</td>\n",
       "      <td>terms exercise regions</td>\n",
       "      <td>exercise regions brain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For example, left-hand</td>\n",
       "      <td>example, left-hand side</td>\n",
       "      <td>left-hand side brain,</td>\n",
       "      <td>side brain, ear,</td>\n",
       "      <td>brain, ear, one</td>\n",
       "      <td>ear, one tiny</td>\n",
       "      <td>one tiny regions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The region responds</td>\n",
       "      <td>region responds “killed”,</td>\n",
       "      <td>responds “killed”, “convicted”,</td>\n",
       "      <td>“killed”, “convicted”, “murdered”</td>\n",
       "      <td>“convicted”, “murdered” “confessed”.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>On brain’s right-hand</td>\n",
       "      <td>brain’s right-hand side,</td>\n",
       "      <td>right-hand side, near</td>\n",
       "      <td>side, near top</td>\n",
       "      <td>near top head,</td>\n",
       "      <td>top head, one</td>\n",
       "      <td>head, one brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Each word represented</td>\n",
       "      <td>word represented one</td>\n",
       "      <td>represented one spot</td>\n",
       "      <td>one spot words</td>\n",
       "      <td>spot words tend</td>\n",
       "      <td>words tend several</td>\n",
       "      <td>tend several meanings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>One part brain,</td>\n",
       "      <td>part brain, example,</td>\n",
       "      <td>brain, example, reliably</td>\n",
       "      <td>example, reliably responds</td>\n",
       "      <td>reliably responds word</td>\n",
       "      <td>responds word “top”,</td>\n",
       "      <td>word “top”, along</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>But word “top”</td>\n",
       "      <td>word “top” activates</td>\n",
       "      <td>“top” activates many</td>\n",
       "      <td>activates many regions.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>One responds numbers</td>\n",
       "      <td>responds numbers measurements,</td>\n",
       "      <td>numbers measurements, another</td>\n",
       "      <td>measurements, another buildings</td>\n",
       "      <td>another buildings places.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The scientists created</td>\n",
       "      <td>scientists created interactive</td>\n",
       "      <td>created interactive website</td>\n",
       "      <td>interactive website public</td>\n",
       "      <td>website public explore</td>\n",
       "      <td>public explore brain</td>\n",
       "      <td>explore brain atlas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Strikingly, brain atlases</td>\n",
       "      <td>brain atlases similar</td>\n",
       "      <td>atlases similar participants,</td>\n",
       "      <td>similar participants, suggesting</td>\n",
       "      <td>participants, suggesting brains</td>\n",
       "      <td>suggesting brains organised</td>\n",
       "      <td>brains organised meanings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The scientists scanned</td>\n",
       "      <td>scientists scanned five</td>\n",
       "      <td>scanned five men</td>\n",
       "      <td>five men two</td>\n",
       "      <td>men two women,</td>\n",
       "      <td>two women, however.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>All native English</td>\n",
       "      <td>native English speakers,</td>\n",
       "      <td>English speakers, two</td>\n",
       "      <td>speakers, two authors</td>\n",
       "      <td>two authors study</td>\n",
       "      <td>authors study published</td>\n",
       "      <td>study published Nature.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It highly possible</td>\n",
       "      <td>highly possible people</td>\n",
       "      <td>possible people different</td>\n",
       "      <td>people different backgrounds</td>\n",
       "      <td>different backgrounds cultures</td>\n",
       "      <td>backgrounds cultures different</td>\n",
       "      <td>cultures different semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Armed atlas, researchers</td>\n",
       "      <td>atlas, researchers piece</td>\n",
       "      <td>researchers piece together</td>\n",
       "      <td>piece together brain</td>\n",
       "      <td>together brain networks</td>\n",
       "      <td>brain networks represent</td>\n",
       "      <td>networks represent wildly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>“The idea murder</td>\n",
       "      <td>idea murder represented</td>\n",
       "      <td>murder represented lot</td>\n",
       "      <td>represented lot brain,”</td>\n",
       "      <td>lot brain,” Gallant</td>\n",
       "      <td>brain,” Gallant said.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Using haul data,</td>\n",
       "      <td>haul data, group</td>\n",
       "      <td>data, group begun</td>\n",
       "      <td>group begun work</td>\n",
       "      <td>begun work new</td>\n",
       "      <td>work new atlases</td>\n",
       "      <td>new atlases show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A brain atlas</td>\n",
       "      <td>brain atlas narrative</td>\n",
       "      <td>atlas narrative structure</td>\n",
       "      <td>narrative structure far</td>\n",
       "      <td>structure far proved</td>\n",
       "      <td>far proved elusive,</td>\n",
       "      <td>proved elusive, however.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>“Every time come</td>\n",
       "      <td>time come set</td>\n",
       "      <td>come set narrative</td>\n",
       "      <td>set narrative features,</td>\n",
       "      <td>narrative features, get</td>\n",
       "      <td>features, get told</td>\n",
       "      <td>get told aren’t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uri Hasson, neuroscientist</td>\n",
       "      <td>Hasson, neuroscientist Princeton</td>\n",
       "      <td>neuroscientist Princeton University,</td>\n",
       "      <td>Princeton University, praised</td>\n",
       "      <td>University, praised work.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Unlike many studies</td>\n",
       "      <td>many studies looked</td>\n",
       "      <td>studies looked brain</td>\n",
       "      <td>looked brain activity</td>\n",
       "      <td>brain activity isolated</td>\n",
       "      <td>activity isolated word</td>\n",
       "      <td>isolated word sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The next step,</td>\n",
       "      <td>next step, said,</td>\n",
       "      <td>step, said, create</td>\n",
       "      <td>said, create comprehensive</td>\n",
       "      <td>create comprehensive precise</td>\n",
       "      <td>comprehensive precise semantic</td>\n",
       "      <td>precise semantic brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ultimately, Hasson believes</td>\n",
       "      <td>Hasson believes possible</td>\n",
       "      <td>believes possible reconstruct</td>\n",
       "      <td>possible reconstruct words</td>\n",
       "      <td>reconstruct words person</td>\n",
       "      <td>words person thinking</td>\n",
       "      <td>person thinking brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The ethical implications</td>\n",
       "      <td>ethical implications enormous.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>One benign use</td>\n",
       "      <td>benign use would</td>\n",
       "      <td>use would see</td>\n",
       "      <td>would see brain</td>\n",
       "      <td>see brain activity</td>\n",
       "      <td>brain activity used</td>\n",
       "      <td>activity used assess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>“There many implications,</td>\n",
       "      <td>many implications, barely</td>\n",
       "      <td>implications, barely touching</td>\n",
       "      <td>barely touching surface,”</td>\n",
       "      <td>touching surface,” said.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Lorraine Tyler, cognitive</td>\n",
       "      <td>Tyler, cognitive neuroscientist</td>\n",
       "      <td>cognitive neuroscientist head</td>\n",
       "      <td>neuroscientist head Centre</td>\n",
       "      <td>head Centre Speech,</td>\n",
       "      <td>Centre Speech, Language</td>\n",
       "      <td>Speech, Language Brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But brain atlas</td>\n",
       "      <td>brain atlas current</td>\n",
       "      <td>atlas current form</td>\n",
       "      <td>current form capture</td>\n",
       "      <td>form capture fine</td>\n",
       "      <td>capture fine differences</td>\n",
       "      <td>fine differences word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>It member many</td>\n",
       "      <td>member many different</td>\n",
       "      <td>many different groups,</td>\n",
       "      <td>different groups, says</td>\n",
       "      <td>groups, says Tyler.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>“It something eat</td>\n",
       "      <td>something eat off,</td>\n",
       "      <td>eat off, things</td>\n",
       "      <td>off, things made</td>\n",
       "      <td>things made wood,</td>\n",
       "      <td>made wood, things</td>\n",
       "      <td>wood, things heavy,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>This kind detailed</td>\n",
       "      <td>kind detailed semantic</td>\n",
       "      <td>detailed semantic information</td>\n",
       "      <td>semantic information enables</td>\n",
       "      <td>information enables words</td>\n",
       "      <td>enables words used</td>\n",
       "      <td>words used flexibly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>“While research path-breaking</td>\n",
       "      <td>research path-breaking scope,</td>\n",
       "      <td>path-breaking scope, still</td>\n",
       "      <td>scope, still lot</td>\n",
       "      <td>still lot learn</td>\n",
       "      <td>lot learn semantics</td>\n",
       "      <td>learn semantics represented</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sample0                           sample1  \\\n",
       "0       Scientists created “atlas             created “atlas brain”   \n",
       "1            Like colourful quilt              colourful quilt laid   \n",
       "2                 “Our goal build                  goal build giant   \n",
       "3                 No single brain               single brain region   \n",
       "4                  A single brain                 single brain spot   \n",
       "5                 And single word                single word lights   \n",
       "6          Together make networks           make networks represent   \n",
       "7             All light networks.                                     \n",
       "8              Described “tour de                   “tour de force”   \n",
       "9       With advances, technology        advances, technology could   \n",
       "10          “It possible approach           possible approach could   \n",
       "11              One potential use               potential use would   \n",
       "12               To create atlas,          create atlas, scientists   \n",
       "13       They matched transcripts       matched transcripts stories   \n",
       "14              Huth used stories                  used stories The   \n",
       "15       The enthralling stories,    enthralling stories, confident   \n",
       "16          Seven people listened               people listened two   \n",
       "17           Per person, amounted          person, amounted hearing   \n",
       "18                The atlas shows                 atlas shows words   \n",
       "19         For example, left-hand           example, left-hand side   \n",
       "20            The region responds         region responds “killed”,   \n",
       "21          On brain’s right-hand          brain’s right-hand side,   \n",
       "22          Each word represented              word represented one   \n",
       "23                One part brain,              part brain, example,   \n",
       "24                 But word “top”              word “top” activates   \n",
       "25           One responds numbers    responds numbers measurements,   \n",
       "26         The scientists created    scientists created interactive   \n",
       "27      Strikingly, brain atlases             brain atlases similar   \n",
       "28         The scientists scanned           scientists scanned five   \n",
       "29             All native English          native English speakers,   \n",
       "30             It highly possible            highly possible people   \n",
       "31       Armed atlas, researchers          atlas, researchers piece   \n",
       "32               “The idea murder           idea murder represented   \n",
       "33               Using haul data,                  haul data, group   \n",
       "34                  A brain atlas             brain atlas narrative   \n",
       "35               “Every time come                     time come set   \n",
       "36     Uri Hasson, neuroscientist  Hasson, neuroscientist Princeton   \n",
       "37            Unlike many studies               many studies looked   \n",
       "38                 The next step,                  next step, said,   \n",
       "39    Ultimately, Hasson believes          Hasson believes possible   \n",
       "40       The ethical implications    ethical implications enormous.   \n",
       "41                 One benign use                  benign use would   \n",
       "42      “There many implications,         many implications, barely   \n",
       "43      Lorraine Tyler, cognitive   Tyler, cognitive neuroscientist   \n",
       "44                But brain atlas               brain atlas current   \n",
       "45                 It member many             member many different   \n",
       "46              “It something eat                something eat off,   \n",
       "47             This kind detailed            kind detailed semantic   \n",
       "48  “While research path-breaking     research path-breaking scope,   \n",
       "\n",
       "                                 sample2                            sample3  \\\n",
       "0                  “atlas brain” reveals            brain” reveals meanings   \n",
       "1                     quilt laid cortex,                 laid cortex, atlas   \n",
       "2                      build giant atlas                  giant atlas shows   \n",
       "3                     brain region holds                   region holds one   \n",
       "4                  brain spot associated             spot associated number   \n",
       "5                       word lights many              lights many different   \n",
       "6            networks represent meanings            represent meanings word   \n",
       "7                                                                             \n",
       "8                          de force” one              force” one researcher   \n",
       "9              technology could profound              could profound impact   \n",
       "10                   approach could used                  could used decode   \n",
       "11                    use would language             would language decoder   \n",
       "12            atlas, scientists recorded       scientists recorded people’s   \n",
       "13             transcripts stories brain             stories brain activity   \n",
       "14                      stories The Moth                     The Moth Radio   \n",
       "15         stories, confident scientists         confident scientists could   \n",
       "16                    listened two hours                  two hours stories   \n",
       "17              amounted hearing roughly             hearing roughly 25,000   \n",
       "18                   shows words related                words related terms   \n",
       "19                 left-hand side brain,                   side brain, ear,   \n",
       "20       responds “killed”, “convicted”,  “killed”, “convicted”, “murdered”   \n",
       "21                 right-hand side, near                     side, near top   \n",
       "22                  represented one spot                     one spot words   \n",
       "23              brain, example, reliably         example, reliably responds   \n",
       "24                  “top” activates many            activates many regions.   \n",
       "25         numbers measurements, another    measurements, another buildings   \n",
       "26           created interactive website         interactive website public   \n",
       "27         atlases similar participants,   similar participants, suggesting   \n",
       "28                      scanned five men                       five men two   \n",
       "29                 English speakers, two              speakers, two authors   \n",
       "30             possible people different       people different backgrounds   \n",
       "31            researchers piece together               piece together brain   \n",
       "32                murder represented lot            represented lot brain,”   \n",
       "33                     data, group begun                   group begun work   \n",
       "34             atlas narrative structure            narrative structure far   \n",
       "35                    come set narrative            set narrative features,   \n",
       "36  neuroscientist Princeton University,      Princeton University, praised   \n",
       "37                  studies looked brain              looked brain activity   \n",
       "38                    step, said, create         said, create comprehensive   \n",
       "39         believes possible reconstruct         possible reconstruct words   \n",
       "40                                                                            \n",
       "41                         use would see                    would see brain   \n",
       "42         implications, barely touching          barely touching surface,”   \n",
       "43         cognitive neuroscientist head         neuroscientist head Centre   \n",
       "44                    atlas current form               current form capture   \n",
       "45                many different groups,             different groups, says   \n",
       "46                       eat off, things                   off, things made   \n",
       "47         detailed semantic information       semantic information enables   \n",
       "48            path-breaking scope, still                   scope, still lot   \n",
       "\n",
       "                                 sample4                         sample5  \\\n",
       "0                 reveals meanings words         meanings words arranged   \n",
       "1                 cortex, atlas displays          atlas displays rainbow   \n",
       "2                        atlas shows one              shows one specific   \n",
       "3                         holds one word               one word concept.   \n",
       "4              associated number related           number related words.   \n",
       "5                   many different brain          different brain spots.   \n",
       "6                     meanings word use:                  word use: life   \n",
       "7                                                                          \n",
       "8                one researcher involved      researcher involved study,   \n",
       "9               profound impact medicine         impact medicine fields.   \n",
       "10               used decode information        decode information words   \n",
       "11                language decoder could             decoder could allow   \n",
       "12               recorded people’s brain         people’s brain activity   \n",
       "13                   brain activity data              activity data show   \n",
       "14                       Moth Radio Hour                Radio Hour short   \n",
       "15               scientists could people            could people scanned   \n",
       "16                   hours stories each.                                   \n",
       "17                 roughly 25,000 words-             25,000 words- 3,000   \n",
       "18                related terms exercise          terms exercise regions   \n",
       "19                       brain, ear, one                   ear, one tiny   \n",
       "20  “convicted”, “murdered” “confessed”.                                   \n",
       "21                        near top head,                   top head, one   \n",
       "22                       spot words tend              words tend several   \n",
       "23                reliably responds word            responds word “top”,   \n",
       "24                                                                         \n",
       "25             another buildings places.                                   \n",
       "26                website public explore            public explore brain   \n",
       "27       participants, suggesting brains     suggesting brains organised   \n",
       "28                        men two women,             two women, however.   \n",
       "29                     two authors study         authors study published   \n",
       "30        different backgrounds cultures  backgrounds cultures different   \n",
       "31               together brain networks        brain networks represent   \n",
       "32                   lot brain,” Gallant           brain,” Gallant said.   \n",
       "33                        begun work new                work new atlases   \n",
       "34                  structure far proved             far proved elusive,   \n",
       "35               narrative features, get              features, get told   \n",
       "36             University, praised work.                                   \n",
       "37               brain activity isolated          activity isolated word   \n",
       "38          create comprehensive precise  comprehensive precise semantic   \n",
       "39              reconstruct words person           words person thinking   \n",
       "40                                                                         \n",
       "41                    see brain activity             brain activity used   \n",
       "42              touching surface,” said.                                   \n",
       "43                   head Centre Speech,         Centre Speech, Language   \n",
       "44                     form capture fine        capture fine differences   \n",
       "45                   groups, says Tyler.                                   \n",
       "46                     things made wood,               made wood, things   \n",
       "47             information enables words              enables words used   \n",
       "48                       still lot learn             lot learn semantics   \n",
       "\n",
       "                        sample6  \n",
       "0         words arranged across  \n",
       "1         displays rainbow hues  \n",
       "2           one specific aspect  \n",
       "3                                \n",
       "4                                \n",
       "5                                \n",
       "6               use: life love;  \n",
       "7                                \n",
       "8         involved study, atlas  \n",
       "9                                \n",
       "10     information words person  \n",
       "11           could allow people  \n",
       "12      brain activity listened  \n",
       "13             data show groups  \n",
       "14       Hour short compelling.  \n",
       "15      people scanned focusing  \n",
       "16                               \n",
       "17       words- 3,000 different  \n",
       "18      exercise regions brain.  \n",
       "19             one tiny regions  \n",
       "20                               \n",
       "21              head, one brain  \n",
       "22       tend several meanings.  \n",
       "23            word “top”, along  \n",
       "24                               \n",
       "25                               \n",
       "26         explore brain atlas.  \n",
       "27    brains organised meanings  \n",
       "28                               \n",
       "29      study published Nature.  \n",
       "30  cultures different semantic  \n",
       "31    networks represent wildly  \n",
       "32                               \n",
       "33             new atlases show  \n",
       "34     proved elusive, however.  \n",
       "35              get told aren’t  \n",
       "36                               \n",
       "37       isolated word sentence  \n",
       "38       precise semantic brain  \n",
       "39        person thinking brain  \n",
       "40                               \n",
       "41         activity used assess  \n",
       "42                               \n",
       "43       Speech, Language Brain  \n",
       "44        fine differences word  \n",
       "45                               \n",
       "46          wood, things heavy,  \n",
       "47          words used flexibly  \n",
       "48  learn semantics represented  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        testmeta1.append(' '.join(x) for x in trigrams(filtered_words))\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0', 'sample1', 'sample2', 'sample3', 'sample4', 'sample5', 'sample6']\n",
    "df_test = df_test.fillna(\"\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>sample1</th>\n",
       "      <th>sample2</th>\n",
       "      <th>sample3</th>\n",
       "      <th>sample4</th>\n",
       "      <th>sample5</th>\n",
       "      <th>sample6</th>\n",
       "      <th>metaphor0</th>\n",
       "      <th>metaphor1</th>\n",
       "      <th>metaphor2</th>\n",
       "      <th>metaphor3</th>\n",
       "      <th>metaphor4</th>\n",
       "      <th>metaphor5</th>\n",
       "      <th>metaphor6</th>\n",
       "      <th>sum_metaphor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientists created “atlas</td>\n",
       "      <td>created “atlas brain”</td>\n",
       "      <td>“atlas brain” reveals</td>\n",
       "      <td>brain” reveals meanings</td>\n",
       "      <td>reveals meanings words</td>\n",
       "      <td>meanings words arranged</td>\n",
       "      <td>words arranged across</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like colourful quilt</td>\n",
       "      <td>colourful quilt laid</td>\n",
       "      <td>quilt laid cortex,</td>\n",
       "      <td>laid cortex, atlas</td>\n",
       "      <td>cortex, atlas displays</td>\n",
       "      <td>atlas displays rainbow</td>\n",
       "      <td>displays rainbow hues</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Our goal build</td>\n",
       "      <td>goal build giant</td>\n",
       "      <td>build giant atlas</td>\n",
       "      <td>giant atlas shows</td>\n",
       "      <td>atlas shows one</td>\n",
       "      <td>shows one specific</td>\n",
       "      <td>one specific aspect</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No single brain</td>\n",
       "      <td>single brain region</td>\n",
       "      <td>brain region holds</td>\n",
       "      <td>region holds one</td>\n",
       "      <td>holds one word</td>\n",
       "      <td>one word concept.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A single brain</td>\n",
       "      <td>single brain spot</td>\n",
       "      <td>brain spot associated</td>\n",
       "      <td>spot associated number</td>\n",
       "      <td>associated number related</td>\n",
       "      <td>number related words.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And single word</td>\n",
       "      <td>single word lights</td>\n",
       "      <td>word lights many</td>\n",
       "      <td>lights many different</td>\n",
       "      <td>many different brain</td>\n",
       "      <td>different brain spots.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Together make networks</td>\n",
       "      <td>make networks represent</td>\n",
       "      <td>networks represent meanings</td>\n",
       "      <td>represent meanings word</td>\n",
       "      <td>meanings word use:</td>\n",
       "      <td>word use: life</td>\n",
       "      <td>use: life love;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>All light networks.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Described “tour de</td>\n",
       "      <td>“tour de force”</td>\n",
       "      <td>de force” one</td>\n",
       "      <td>force” one researcher</td>\n",
       "      <td>one researcher involved</td>\n",
       "      <td>researcher involved study,</td>\n",
       "      <td>involved study, atlas</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>With advances, technology</td>\n",
       "      <td>advances, technology could</td>\n",
       "      <td>technology could profound</td>\n",
       "      <td>could profound impact</td>\n",
       "      <td>profound impact medicine</td>\n",
       "      <td>impact medicine fields.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>“It possible approach</td>\n",
       "      <td>possible approach could</td>\n",
       "      <td>approach could used</td>\n",
       "      <td>could used decode</td>\n",
       "      <td>used decode information</td>\n",
       "      <td>decode information words</td>\n",
       "      <td>information words person</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One potential use</td>\n",
       "      <td>potential use would</td>\n",
       "      <td>use would language</td>\n",
       "      <td>would language decoder</td>\n",
       "      <td>language decoder could</td>\n",
       "      <td>decoder could allow</td>\n",
       "      <td>could allow people</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>To create atlas,</td>\n",
       "      <td>create atlas, scientists</td>\n",
       "      <td>atlas, scientists recorded</td>\n",
       "      <td>scientists recorded people’s</td>\n",
       "      <td>recorded people’s brain</td>\n",
       "      <td>people’s brain activity</td>\n",
       "      <td>brain activity listened</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>They matched transcripts</td>\n",
       "      <td>matched transcripts stories</td>\n",
       "      <td>transcripts stories brain</td>\n",
       "      <td>stories brain activity</td>\n",
       "      <td>brain activity data</td>\n",
       "      <td>activity data show</td>\n",
       "      <td>data show groups</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Huth used stories</td>\n",
       "      <td>used stories The</td>\n",
       "      <td>stories The Moth</td>\n",
       "      <td>The Moth Radio</td>\n",
       "      <td>Moth Radio Hour</td>\n",
       "      <td>Radio Hour short</td>\n",
       "      <td>Hour short compelling.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The enthralling stories,</td>\n",
       "      <td>enthralling stories, confident</td>\n",
       "      <td>stories, confident scientists</td>\n",
       "      <td>confident scientists could</td>\n",
       "      <td>scientists could people</td>\n",
       "      <td>could people scanned</td>\n",
       "      <td>people scanned focusing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Seven people listened</td>\n",
       "      <td>people listened two</td>\n",
       "      <td>listened two hours</td>\n",
       "      <td>two hours stories</td>\n",
       "      <td>hours stories each.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Per person, amounted</td>\n",
       "      <td>person, amounted hearing</td>\n",
       "      <td>amounted hearing roughly</td>\n",
       "      <td>hearing roughly 25,000</td>\n",
       "      <td>roughly 25,000 words-</td>\n",
       "      <td>25,000 words- 3,000</td>\n",
       "      <td>words- 3,000 different</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The atlas shows</td>\n",
       "      <td>atlas shows words</td>\n",
       "      <td>shows words related</td>\n",
       "      <td>words related terms</td>\n",
       "      <td>related terms exercise</td>\n",
       "      <td>terms exercise regions</td>\n",
       "      <td>exercise regions brain.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For example, left-hand</td>\n",
       "      <td>example, left-hand side</td>\n",
       "      <td>left-hand side brain,</td>\n",
       "      <td>side brain, ear,</td>\n",
       "      <td>brain, ear, one</td>\n",
       "      <td>ear, one tiny</td>\n",
       "      <td>one tiny regions</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The region responds</td>\n",
       "      <td>region responds “killed”,</td>\n",
       "      <td>responds “killed”, “convicted”,</td>\n",
       "      <td>“killed”, “convicted”, “murdered”</td>\n",
       "      <td>“convicted”, “murdered” “confessed”.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>On brain’s right-hand</td>\n",
       "      <td>brain’s right-hand side,</td>\n",
       "      <td>right-hand side, near</td>\n",
       "      <td>side, near top</td>\n",
       "      <td>near top head,</td>\n",
       "      <td>top head, one</td>\n",
       "      <td>head, one brain</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Each word represented</td>\n",
       "      <td>word represented one</td>\n",
       "      <td>represented one spot</td>\n",
       "      <td>one spot words</td>\n",
       "      <td>spot words tend</td>\n",
       "      <td>words tend several</td>\n",
       "      <td>tend several meanings.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>One part brain,</td>\n",
       "      <td>part brain, example,</td>\n",
       "      <td>brain, example, reliably</td>\n",
       "      <td>example, reliably responds</td>\n",
       "      <td>reliably responds word</td>\n",
       "      <td>responds word “top”,</td>\n",
       "      <td>word “top”, along</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>But word “top”</td>\n",
       "      <td>word “top” activates</td>\n",
       "      <td>“top” activates many</td>\n",
       "      <td>activates many regions.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>One responds numbers</td>\n",
       "      <td>responds numbers measurements,</td>\n",
       "      <td>numbers measurements, another</td>\n",
       "      <td>measurements, another buildings</td>\n",
       "      <td>another buildings places.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The scientists created</td>\n",
       "      <td>scientists created interactive</td>\n",
       "      <td>created interactive website</td>\n",
       "      <td>interactive website public</td>\n",
       "      <td>website public explore</td>\n",
       "      <td>public explore brain</td>\n",
       "      <td>explore brain atlas.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Strikingly, brain atlases</td>\n",
       "      <td>brain atlases similar</td>\n",
       "      <td>atlases similar participants,</td>\n",
       "      <td>similar participants, suggesting</td>\n",
       "      <td>participants, suggesting brains</td>\n",
       "      <td>suggesting brains organised</td>\n",
       "      <td>brains organised meanings</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The scientists scanned</td>\n",
       "      <td>scientists scanned five</td>\n",
       "      <td>scanned five men</td>\n",
       "      <td>five men two</td>\n",
       "      <td>men two women,</td>\n",
       "      <td>two women, however.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>All native English</td>\n",
       "      <td>native English speakers,</td>\n",
       "      <td>English speakers, two</td>\n",
       "      <td>speakers, two authors</td>\n",
       "      <td>two authors study</td>\n",
       "      <td>authors study published</td>\n",
       "      <td>study published Nature.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It highly possible</td>\n",
       "      <td>highly possible people</td>\n",
       "      <td>possible people different</td>\n",
       "      <td>people different backgrounds</td>\n",
       "      <td>different backgrounds cultures</td>\n",
       "      <td>backgrounds cultures different</td>\n",
       "      <td>cultures different semantic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Armed atlas, researchers</td>\n",
       "      <td>atlas, researchers piece</td>\n",
       "      <td>researchers piece together</td>\n",
       "      <td>piece together brain</td>\n",
       "      <td>together brain networks</td>\n",
       "      <td>brain networks represent</td>\n",
       "      <td>networks represent wildly</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>“The idea murder</td>\n",
       "      <td>idea murder represented</td>\n",
       "      <td>murder represented lot</td>\n",
       "      <td>represented lot brain,”</td>\n",
       "      <td>lot brain,” Gallant</td>\n",
       "      <td>brain,” Gallant said.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Using haul data,</td>\n",
       "      <td>haul data, group</td>\n",
       "      <td>data, group begun</td>\n",
       "      <td>group begun work</td>\n",
       "      <td>begun work new</td>\n",
       "      <td>work new atlases</td>\n",
       "      <td>new atlases show</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A brain atlas</td>\n",
       "      <td>brain atlas narrative</td>\n",
       "      <td>atlas narrative structure</td>\n",
       "      <td>narrative structure far</td>\n",
       "      <td>structure far proved</td>\n",
       "      <td>far proved elusive,</td>\n",
       "      <td>proved elusive, however.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>“Every time come</td>\n",
       "      <td>time come set</td>\n",
       "      <td>come set narrative</td>\n",
       "      <td>set narrative features,</td>\n",
       "      <td>narrative features, get</td>\n",
       "      <td>features, get told</td>\n",
       "      <td>get told aren’t</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uri Hasson, neuroscientist</td>\n",
       "      <td>Hasson, neuroscientist Princeton</td>\n",
       "      <td>neuroscientist Princeton University,</td>\n",
       "      <td>Princeton University, praised</td>\n",
       "      <td>University, praised work.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Unlike many studies</td>\n",
       "      <td>many studies looked</td>\n",
       "      <td>studies looked brain</td>\n",
       "      <td>looked brain activity</td>\n",
       "      <td>brain activity isolated</td>\n",
       "      <td>activity isolated word</td>\n",
       "      <td>isolated word sentence</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The next step,</td>\n",
       "      <td>next step, said,</td>\n",
       "      <td>step, said, create</td>\n",
       "      <td>said, create comprehensive</td>\n",
       "      <td>create comprehensive precise</td>\n",
       "      <td>comprehensive precise semantic</td>\n",
       "      <td>precise semantic brain</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ultimately, Hasson believes</td>\n",
       "      <td>Hasson believes possible</td>\n",
       "      <td>believes possible reconstruct</td>\n",
       "      <td>possible reconstruct words</td>\n",
       "      <td>reconstruct words person</td>\n",
       "      <td>words person thinking</td>\n",
       "      <td>person thinking brain</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The ethical implications</td>\n",
       "      <td>ethical implications enormous.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>One benign use</td>\n",
       "      <td>benign use would</td>\n",
       "      <td>use would see</td>\n",
       "      <td>would see brain</td>\n",
       "      <td>see brain activity</td>\n",
       "      <td>brain activity used</td>\n",
       "      <td>activity used assess</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>“There many implications,</td>\n",
       "      <td>many implications, barely</td>\n",
       "      <td>implications, barely touching</td>\n",
       "      <td>barely touching surface,”</td>\n",
       "      <td>touching surface,” said.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Lorraine Tyler, cognitive</td>\n",
       "      <td>Tyler, cognitive neuroscientist</td>\n",
       "      <td>cognitive neuroscientist head</td>\n",
       "      <td>neuroscientist head Centre</td>\n",
       "      <td>head Centre Speech,</td>\n",
       "      <td>Centre Speech, Language</td>\n",
       "      <td>Speech, Language Brain</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But brain atlas</td>\n",
       "      <td>brain atlas current</td>\n",
       "      <td>atlas current form</td>\n",
       "      <td>current form capture</td>\n",
       "      <td>form capture fine</td>\n",
       "      <td>capture fine differences</td>\n",
       "      <td>fine differences word</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>It member many</td>\n",
       "      <td>member many different</td>\n",
       "      <td>many different groups,</td>\n",
       "      <td>different groups, says</td>\n",
       "      <td>groups, says Tyler.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>“It something eat</td>\n",
       "      <td>something eat off,</td>\n",
       "      <td>eat off, things</td>\n",
       "      <td>off, things made</td>\n",
       "      <td>things made wood,</td>\n",
       "      <td>made wood, things</td>\n",
       "      <td>wood, things heavy,</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>This kind detailed</td>\n",
       "      <td>kind detailed semantic</td>\n",
       "      <td>detailed semantic information</td>\n",
       "      <td>semantic information enables</td>\n",
       "      <td>information enables words</td>\n",
       "      <td>enables words used</td>\n",
       "      <td>words used flexibly</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>“While research path-breaking</td>\n",
       "      <td>research path-breaking scope,</td>\n",
       "      <td>path-breaking scope, still</td>\n",
       "      <td>scope, still lot</td>\n",
       "      <td>still lot learn</td>\n",
       "      <td>lot learn semantics</td>\n",
       "      <td>learn semantics represented</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sample0                           sample1  \\\n",
       "0       Scientists created “atlas             created “atlas brain”   \n",
       "1            Like colourful quilt              colourful quilt laid   \n",
       "2                 “Our goal build                  goal build giant   \n",
       "3                 No single brain               single brain region   \n",
       "4                  A single brain                 single brain spot   \n",
       "5                 And single word                single word lights   \n",
       "6          Together make networks           make networks represent   \n",
       "7             All light networks.                                     \n",
       "8              Described “tour de                   “tour de force”   \n",
       "9       With advances, technology        advances, technology could   \n",
       "10          “It possible approach           possible approach could   \n",
       "11              One potential use               potential use would   \n",
       "12               To create atlas,          create atlas, scientists   \n",
       "13       They matched transcripts       matched transcripts stories   \n",
       "14              Huth used stories                  used stories The   \n",
       "15       The enthralling stories,    enthralling stories, confident   \n",
       "16          Seven people listened               people listened two   \n",
       "17           Per person, amounted          person, amounted hearing   \n",
       "18                The atlas shows                 atlas shows words   \n",
       "19         For example, left-hand           example, left-hand side   \n",
       "20            The region responds         region responds “killed”,   \n",
       "21          On brain’s right-hand          brain’s right-hand side,   \n",
       "22          Each word represented              word represented one   \n",
       "23                One part brain,              part brain, example,   \n",
       "24                 But word “top”              word “top” activates   \n",
       "25           One responds numbers    responds numbers measurements,   \n",
       "26         The scientists created    scientists created interactive   \n",
       "27      Strikingly, brain atlases             brain atlases similar   \n",
       "28         The scientists scanned           scientists scanned five   \n",
       "29             All native English          native English speakers,   \n",
       "30             It highly possible            highly possible people   \n",
       "31       Armed atlas, researchers          atlas, researchers piece   \n",
       "32               “The idea murder           idea murder represented   \n",
       "33               Using haul data,                  haul data, group   \n",
       "34                  A brain atlas             brain atlas narrative   \n",
       "35               “Every time come                     time come set   \n",
       "36     Uri Hasson, neuroscientist  Hasson, neuroscientist Princeton   \n",
       "37            Unlike many studies               many studies looked   \n",
       "38                 The next step,                  next step, said,   \n",
       "39    Ultimately, Hasson believes          Hasson believes possible   \n",
       "40       The ethical implications    ethical implications enormous.   \n",
       "41                 One benign use                  benign use would   \n",
       "42      “There many implications,         many implications, barely   \n",
       "43      Lorraine Tyler, cognitive   Tyler, cognitive neuroscientist   \n",
       "44                But brain atlas               brain atlas current   \n",
       "45                 It member many             member many different   \n",
       "46              “It something eat                something eat off,   \n",
       "47             This kind detailed            kind detailed semantic   \n",
       "48  “While research path-breaking     research path-breaking scope,   \n",
       "\n",
       "                                 sample2                            sample3  \\\n",
       "0                  “atlas brain” reveals            brain” reveals meanings   \n",
       "1                     quilt laid cortex,                 laid cortex, atlas   \n",
       "2                      build giant atlas                  giant atlas shows   \n",
       "3                     brain region holds                   region holds one   \n",
       "4                  brain spot associated             spot associated number   \n",
       "5                       word lights many              lights many different   \n",
       "6            networks represent meanings            represent meanings word   \n",
       "7                                                                             \n",
       "8                          de force” one              force” one researcher   \n",
       "9              technology could profound              could profound impact   \n",
       "10                   approach could used                  could used decode   \n",
       "11                    use would language             would language decoder   \n",
       "12            atlas, scientists recorded       scientists recorded people’s   \n",
       "13             transcripts stories brain             stories brain activity   \n",
       "14                      stories The Moth                     The Moth Radio   \n",
       "15         stories, confident scientists         confident scientists could   \n",
       "16                    listened two hours                  two hours stories   \n",
       "17              amounted hearing roughly             hearing roughly 25,000   \n",
       "18                   shows words related                words related terms   \n",
       "19                 left-hand side brain,                   side brain, ear,   \n",
       "20       responds “killed”, “convicted”,  “killed”, “convicted”, “murdered”   \n",
       "21                 right-hand side, near                     side, near top   \n",
       "22                  represented one spot                     one spot words   \n",
       "23              brain, example, reliably         example, reliably responds   \n",
       "24                  “top” activates many            activates many regions.   \n",
       "25         numbers measurements, another    measurements, another buildings   \n",
       "26           created interactive website         interactive website public   \n",
       "27         atlases similar participants,   similar participants, suggesting   \n",
       "28                      scanned five men                       five men two   \n",
       "29                 English speakers, two              speakers, two authors   \n",
       "30             possible people different       people different backgrounds   \n",
       "31            researchers piece together               piece together brain   \n",
       "32                murder represented lot            represented lot brain,”   \n",
       "33                     data, group begun                   group begun work   \n",
       "34             atlas narrative structure            narrative structure far   \n",
       "35                    come set narrative            set narrative features,   \n",
       "36  neuroscientist Princeton University,      Princeton University, praised   \n",
       "37                  studies looked brain              looked brain activity   \n",
       "38                    step, said, create         said, create comprehensive   \n",
       "39         believes possible reconstruct         possible reconstruct words   \n",
       "40                                                                            \n",
       "41                         use would see                    would see brain   \n",
       "42         implications, barely touching          barely touching surface,”   \n",
       "43         cognitive neuroscientist head         neuroscientist head Centre   \n",
       "44                    atlas current form               current form capture   \n",
       "45                many different groups,             different groups, says   \n",
       "46                       eat off, things                   off, things made   \n",
       "47         detailed semantic information       semantic information enables   \n",
       "48            path-breaking scope, still                   scope, still lot   \n",
       "\n",
       "                                 sample4                         sample5  \\\n",
       "0                 reveals meanings words         meanings words arranged   \n",
       "1                 cortex, atlas displays          atlas displays rainbow   \n",
       "2                        atlas shows one              shows one specific   \n",
       "3                         holds one word               one word concept.   \n",
       "4              associated number related           number related words.   \n",
       "5                   many different brain          different brain spots.   \n",
       "6                     meanings word use:                  word use: life   \n",
       "7                                                                          \n",
       "8                one researcher involved      researcher involved study,   \n",
       "9               profound impact medicine         impact medicine fields.   \n",
       "10               used decode information        decode information words   \n",
       "11                language decoder could             decoder could allow   \n",
       "12               recorded people’s brain         people’s brain activity   \n",
       "13                   brain activity data              activity data show   \n",
       "14                       Moth Radio Hour                Radio Hour short   \n",
       "15               scientists could people            could people scanned   \n",
       "16                   hours stories each.                                   \n",
       "17                 roughly 25,000 words-             25,000 words- 3,000   \n",
       "18                related terms exercise          terms exercise regions   \n",
       "19                       brain, ear, one                   ear, one tiny   \n",
       "20  “convicted”, “murdered” “confessed”.                                   \n",
       "21                        near top head,                   top head, one   \n",
       "22                       spot words tend              words tend several   \n",
       "23                reliably responds word            responds word “top”,   \n",
       "24                                                                         \n",
       "25             another buildings places.                                   \n",
       "26                website public explore            public explore brain   \n",
       "27       participants, suggesting brains     suggesting brains organised   \n",
       "28                        men two women,             two women, however.   \n",
       "29                     two authors study         authors study published   \n",
       "30        different backgrounds cultures  backgrounds cultures different   \n",
       "31               together brain networks        brain networks represent   \n",
       "32                   lot brain,” Gallant           brain,” Gallant said.   \n",
       "33                        begun work new                work new atlases   \n",
       "34                  structure far proved             far proved elusive,   \n",
       "35               narrative features, get              features, get told   \n",
       "36             University, praised work.                                   \n",
       "37               brain activity isolated          activity isolated word   \n",
       "38          create comprehensive precise  comprehensive precise semantic   \n",
       "39              reconstruct words person           words person thinking   \n",
       "40                                                                         \n",
       "41                    see brain activity             brain activity used   \n",
       "42              touching surface,” said.                                   \n",
       "43                   head Centre Speech,         Centre Speech, Language   \n",
       "44                     form capture fine        capture fine differences   \n",
       "45                   groups, says Tyler.                                   \n",
       "46                     things made wood,               made wood, things   \n",
       "47             information enables words              enables words used   \n",
       "48                       still lot learn             lot learn semantics   \n",
       "\n",
       "                        sample6  metaphor0  metaphor1  metaphor2  metaphor3  \\\n",
       "0         words arranged across          1          1          1          1   \n",
       "1         displays rainbow hues          1          1          1          1   \n",
       "2           one specific aspect          1          1          1          1   \n",
       "3                                        1          1          1          1   \n",
       "4                                        1          1          1          1   \n",
       "5                                        1          1          1          1   \n",
       "6               use: life love;          1          1          1          1   \n",
       "7                                        1          1          1          1   \n",
       "8         involved study, atlas          1          1          1          1   \n",
       "9                                        1          1          1          1   \n",
       "10     information words person          1          1          1          1   \n",
       "11           could allow people          1          1          1          1   \n",
       "12      brain activity listened          1          1          1          1   \n",
       "13             data show groups          1          1          1          1   \n",
       "14       Hour short compelling.          1          1          1          1   \n",
       "15      people scanned focusing          1          1          1          1   \n",
       "16                                       1          1          1          1   \n",
       "17       words- 3,000 different          1          1          1          1   \n",
       "18      exercise regions brain.          1          1          1          1   \n",
       "19             one tiny regions          1          1          1          1   \n",
       "20                                       1          1          1          1   \n",
       "21              head, one brain          1          1          1          1   \n",
       "22       tend several meanings.          1          1          1          1   \n",
       "23            word “top”, along          1          1          1          1   \n",
       "24                                       1          1          1          1   \n",
       "25                                       1          1          1          1   \n",
       "26         explore brain atlas.          1          1          1          1   \n",
       "27    brains organised meanings          1          1          1          1   \n",
       "28                                       1          1          1          1   \n",
       "29      study published Nature.          1          1          1          1   \n",
       "30  cultures different semantic          1          1          1          1   \n",
       "31    networks represent wildly          1          1          1          1   \n",
       "32                                       1          1          1          1   \n",
       "33             new atlases show          1          1          1          1   \n",
       "34     proved elusive, however.          1          1          1          1   \n",
       "35              get told aren’t          1          1          1          1   \n",
       "36                                       1          1          1          1   \n",
       "37       isolated word sentence          1          1          1          1   \n",
       "38       precise semantic brain          1          1          1          1   \n",
       "39        person thinking brain          1          1          1          1   \n",
       "40                                       1          1          1          1   \n",
       "41         activity used assess          1          1          1          1   \n",
       "42                                       1          1          1          1   \n",
       "43       Speech, Language Brain          1          1          1          1   \n",
       "44        fine differences word          1          1          1          1   \n",
       "45                                       1          1          1          1   \n",
       "46          wood, things heavy,          1          1          1          1   \n",
       "47          words used flexibly          1          1          1          1   \n",
       "48  learn semantics represented          1          1          1          1   \n",
       "\n",
       "    metaphor4  metaphor5  metaphor6  sum_metaphor  \n",
       "0           1          1          1             1  \n",
       "1           1          1          1             1  \n",
       "2           1          1          1             1  \n",
       "3           1          1          1             1  \n",
       "4           1          1          1             1  \n",
       "5           1          1          1             1  \n",
       "6           1          1          1             1  \n",
       "7           1          1          1             1  \n",
       "8           1          1          1             1  \n",
       "9           1          1          1             1  \n",
       "10          1          1          1             1  \n",
       "11          1          1          1             1  \n",
       "12          1          1          1             1  \n",
       "13          1          1          1             1  \n",
       "14          1          1          1             1  \n",
       "15          1          1          1             1  \n",
       "16          1          1          1             1  \n",
       "17          1          1          1             1  \n",
       "18          1          1          1             1  \n",
       "19          1          1          1             1  \n",
       "20          1          1          1             1  \n",
       "21          1          1          1             1  \n",
       "22          1          1          1             1  \n",
       "23          1          1          1             1  \n",
       "24          1          1          1             1  \n",
       "25          1          1          1             1  \n",
       "26          1          1          1             1  \n",
       "27          1          1          1             1  \n",
       "28          1          1          1             1  \n",
       "29          1          1          1             1  \n",
       "30          1          1          1             1  \n",
       "31          1          1          1             1  \n",
       "32          1          1          1             1  \n",
       "33          1          1          1             1  \n",
       "34          1          1          1             1  \n",
       "35          1          1          1             1  \n",
       "36          1          1          1             1  \n",
       "37          1          1          1             1  \n",
       "38          1          1          1             1  \n",
       "39          1          1          1             1  \n",
       "40          1          1          1             1  \n",
       "41          1          1          1             1  \n",
       "42          1          1          1             1  \n",
       "43          1          1          1             1  \n",
       "44          1          1          1             1  \n",
       "45          1          1          1             1  \n",
       "46          1          1          1             1  \n",
       "47          1          1          1             1  \n",
       "48          1          1          1             1  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample1'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor1'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample2'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor2'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample3'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor3'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample4'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor4'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample5'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor5'] = logreg_predictions\n",
    "\n",
    "arr_test_feature_sparse = vec.transform(df_test['sample6'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor6'] = logreg_predictions\n",
    "\n",
    "df_test['sum_metaphor'] = df_test['metaphor1'] + df_test['metaphor2'] + df_test['metaphor3'] +df_test['metaphor4'] +df_test['metaphor5'] \n",
    "\n",
    "if sum(df_test['sum_metaphor']) >= 1:\n",
    "    df_test['sum_metaphor'] = 1\n",
    "sum(df_test['sum_metaphor'])\n",
    "#len(df_test['sum_metaphor'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        testmeta1.append(filtered_words)\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>metaphor0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientists created “atlas brain” reveals meani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like colourful quilt laid cortex, atlas displa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“our goal build giant atlas shows one specific...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no single brain region holds one word concept.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a single brain spot associated number related ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and single word lights many different brain sp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>together make networks represent meanings word...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>all light networks.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>described “tour de force” one researcher invol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>with advances, technology could profound impac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>“it possible approach could used decode inform...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>one potential use would language decoder could...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>to create atlas, scientists recorded people’s ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>they matched transcripts stories brain activit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>huth used stories the moth radio hour short co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the enthralling stories, confident scientists ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>seven people listened two hours stories each.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>per person, amounted hearing roughly 25,000 wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the atlas shows words related terms exercise r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>for example, left-hand side brain, ear, one ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the region responds “killed”, “convicted”, “mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>on brain’s right-hand side, near top head, one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>each word represented one spot words tend seve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>one part brain, example, reliably responds wor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>but word “top” activates many regions.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>one responds numbers measurements, another bui...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the scientists created interactive website pub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>strikingly, brain atlases similar participants...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the scientists scanned five men two women, how...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>all native english speakers, two authors study...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>it highly possible people different background...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>armed atlas, researchers piece together brain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>“the idea murder represented lot brain,” galla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>using haul data, group begun work new atlases ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>a brain atlas narrative structure far proved e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>“every time come set narrative features, get t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>uri hasson, neuroscientist princeton universit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>unlike many studies looked brain activity isol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the next step, said, create comprehensive prec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ultimately, hasson believes possible reconstru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>the ethical implications enormous.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>one benign use would see brain activity used a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>“there many implications, barely touching surf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>lorraine tyler, cognitive neuroscientist head ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>but brain atlas current form capture fine diff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>it member many different groups, says tyler.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>“it something eat off, things made wood, thing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>this kind detailed semantic information enable...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>“while research path-breaking scope, still lot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sample0  metaphor0\n",
       "0   scientists created “atlas brain” reveals meani...          1\n",
       "1   like colourful quilt laid cortex, atlas displa...          1\n",
       "2   “our goal build giant atlas shows one specific...          1\n",
       "3      no single brain region holds one word concept.          1\n",
       "4   a single brain spot associated number related ...          1\n",
       "5   and single word lights many different brain sp...          1\n",
       "6   together make networks represent meanings word...          1\n",
       "7                                 all light networks.          1\n",
       "8   described “tour de force” one researcher invol...          1\n",
       "9   with advances, technology could profound impac...          1\n",
       "10  “it possible approach could used decode inform...          1\n",
       "11  one potential use would language decoder could...          1\n",
       "12  to create atlas, scientists recorded people’s ...          1\n",
       "13  they matched transcripts stories brain activit...          1\n",
       "14  huth used stories the moth radio hour short co...          1\n",
       "15  the enthralling stories, confident scientists ...          1\n",
       "16      seven people listened two hours stories each.          1\n",
       "17  per person, amounted hearing roughly 25,000 wo...          1\n",
       "18  the atlas shows words related terms exercise r...          1\n",
       "19  for example, left-hand side brain, ear, one ti...          1\n",
       "20  the region responds “killed”, “convicted”, “mu...          1\n",
       "21  on brain’s right-hand side, near top head, one...          1\n",
       "22  each word represented one spot words tend seve...          1\n",
       "23  one part brain, example, reliably responds wor...          1\n",
       "24             but word “top” activates many regions.          1\n",
       "25  one responds numbers measurements, another bui...          1\n",
       "26  the scientists created interactive website pub...          1\n",
       "27  strikingly, brain atlases similar participants...          1\n",
       "28  the scientists scanned five men two women, how...          1\n",
       "29  all native english speakers, two authors study...          1\n",
       "30  it highly possible people different background...          1\n",
       "31  armed atlas, researchers piece together brain ...          1\n",
       "32  “the idea murder represented lot brain,” galla...          1\n",
       "33  using haul data, group begun work new atlases ...          1\n",
       "34  a brain atlas narrative structure far proved e...          1\n",
       "35  “every time come set narrative features, get t...          1\n",
       "36  uri hasson, neuroscientist princeton universit...          1\n",
       "37  unlike many studies looked brain activity isol...          1\n",
       "38  the next step, said, create comprehensive prec...          1\n",
       "39  ultimately, hasson believes possible reconstru...          1\n",
       "40                 the ethical implications enormous.          1\n",
       "41  one benign use would see brain activity used a...          1\n",
       "42  “there many implications, barely touching surf...          1\n",
       "43  lorraine tyler, cognitive neuroscientist head ...          1\n",
       "44  but brain atlas current form capture fine diff...          1\n",
       "45       it member many different groups, says tyler.          1\n",
       "46  “it something eat off, things made wood, thing...          1\n",
       "47  this kind detailed semantic information enable...          1\n",
       "48  “while research path-breaking scope, still lot...          1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_test_feature_sparse = vec.transform(df_test['sample0'])\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, df_train['metaphor'])\n",
    "logreg_predictions = logreg_model.predict(arr_test_feature)\n",
    "indices = [i for i, x in enumerate(logreg_predictions) if x == 0]\n",
    "df_test['metaphor0'] = logreg_predictions\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample0</th>\n",
       "      <th>sample1</th>\n",
       "      <th>sample2</th>\n",
       "      <th>sample3</th>\n",
       "      <th>sample4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientists created “atlas</td>\n",
       "      <td>created “atlas brain”</td>\n",
       "      <td>“atlas brain” reveals</td>\n",
       "      <td>brain” reveals meanings</td>\n",
       "      <td>reveals meanings words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like colourful quilt</td>\n",
       "      <td>colourful quilt laid</td>\n",
       "      <td>quilt laid cortex,</td>\n",
       "      <td>laid cortex, atlas</td>\n",
       "      <td>cortex, atlas displays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Our goal build</td>\n",
       "      <td>goal build giant</td>\n",
       "      <td>build giant atlas</td>\n",
       "      <td>giant atlas shows</td>\n",
       "      <td>atlas shows one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No single brain</td>\n",
       "      <td>single brain region</td>\n",
       "      <td>brain region holds</td>\n",
       "      <td>region holds one</td>\n",
       "      <td>holds one word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A single brain</td>\n",
       "      <td>single brain spot</td>\n",
       "      <td>brain spot associated</td>\n",
       "      <td>spot associated number</td>\n",
       "      <td>associated number related</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sample0                sample1                sample2  \\\n",
       "0  Scientists created “atlas  created “atlas brain”  “atlas brain” reveals   \n",
       "1       Like colourful quilt   colourful quilt laid     quilt laid cortex,   \n",
       "2            “Our goal build       goal build giant      build giant atlas   \n",
       "3            No single brain    single brain region     brain region holds   \n",
       "4             A single brain      single brain spot  brain spot associated   \n",
       "\n",
       "                   sample3                    sample4  \n",
       "0  brain” reveals meanings     reveals meanings words  \n",
       "1       laid cortex, atlas     cortex, atlas displays  \n",
       "2        giant atlas shows            atlas shows one  \n",
       "3         region holds one             holds one word  \n",
       "4   spot associated number  associated number related  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.ix[:, 0:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test.columns)\n",
    "#df_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        testmeta1.append(filtered_words)\n",
    "df_test = pd.DataFrame(testmeta1)\n",
    "df_test\n",
    "# df_test = df_test.ix[:, 0:6]\n",
    "df_test.columns = ['sample0']\n",
    "df_test = df_test.fillna(\"\")\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_tagger_on_brown():\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "    return train_tagger(brown_tagged_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def create_corpus(f):\n",
    "    with open(f, 'r') as text_file:\n",
    "        new_corpus = text_file.read()\n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "brown_tagger = train_tagger_on_brown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('scientists', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('created', 'VBN'),\n",
       "  ('an', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('reveals', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('arranged', 'VBN'),\n",
       "  ('across', 'IN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('organ', 'NN')],\n",
       " [('like', 'CS'),\n",
       "  ('a', 'AT'),\n",
       "  ('colourful', 'NN'),\n",
       "  ('quilt', 'NN'),\n",
       "  ('laid', 'VBN'),\n",
       "  ('over', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('cortex', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('displays', 'VBZ'),\n",
       "  ('in', 'IN'),\n",
       "  ('rainbow', 'NN'),\n",
       "  ('hues', 'NNS'),\n",
       "  ('how', 'WRB'),\n",
       "  ('individual', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('concepts', 'NNS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('convey', 'VB'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('grouped', 'VBN'),\n",
       "  ('together', 'RB'),\n",
       "  ('in', 'IN'),\n",
       "  ('clumps', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('white', 'JJ'),\n",
       "  ('matter', 'NN')],\n",
       " [('our', 'PP$'),\n",
       "  ('goal', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('build', 'VB'),\n",
       "  ('a', 'AT'),\n",
       "  ('giant', 'JJ'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('shows', 'NNS'),\n",
       "  ('how', 'WRB'),\n",
       "  ('one', 'PN'),\n",
       "  ('specific', 'JJ'),\n",
       "  ('aspect', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('language', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('case', 'NN'),\n",
       "  ('semantics', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('jack', 'NN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('a', 'AT'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('university', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('california', 'NN'),\n",
       "  ('berkeley', 'NN')],\n",
       " [('no', 'AT'),\n",
       "  ('single', 'AP'),\n",
       "  ('brain', 'NN'),\n",
       "  ('region', 'NN'),\n",
       "  ('holds', 'VBZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('word', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('concept', 'NN')],\n",
       " [('a', 'AT'),\n",
       "  ('single', 'AP'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spot', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('associated', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('number', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('related', 'VBN'),\n",
       "  ('words', 'NNS')],\n",
       " [('and', 'CC'),\n",
       "  ('each', 'DT'),\n",
       "  ('single', 'AP'),\n",
       "  ('word', 'NN'),\n",
       "  ('lights', 'NNS'),\n",
       "  ('up', 'RP'),\n",
       "  ('many', 'AP'),\n",
       "  ('different', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spots', 'NNS')],\n",
       " [('together', 'RB'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('make', 'VB'),\n",
       "  ('up', 'RP'),\n",
       "  ('networks', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represent', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('each', 'DT'),\n",
       "  ('word', 'NN'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('use', 'VB'),\n",
       "  ('life', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('love', 'NN'),\n",
       "  ('death', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('taxes', 'NNS'),\n",
       "  ('clouds', 'NNS'),\n",
       "  ('florida', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bra', 'NN')],\n",
       " [('all', 'ABN'),\n",
       "  ('light', 'NN'),\n",
       "  ('up', 'RP'),\n",
       "  ('their', 'PP$'),\n",
       "  ('own', 'JJ'),\n",
       "  ('networks', 'NNS')],\n",
       " [('described', 'VBN'),\n",
       "  ('as', 'CS'),\n",
       "  ('a', 'AT'),\n",
       "  ('tour', 'NN'),\n",
       "  ('de', 'FW-IN'),\n",
       "  ('force', 'FW-NN'),\n",
       "  ('by', 'IN'),\n",
       "  ('one', 'CD'),\n",
       "  ('researcher', 'NN'),\n",
       "  ('who', 'WPS'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('not', '*'),\n",
       "  ('involved', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('demonstrates', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('modern', 'JJ'),\n",
       "  ('imaging', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('transform', 'VB'),\n",
       "  ('our', 'PP$'),\n",
       "  ('knowledge', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('performs', 'VBZ'),\n",
       "  ('some', 'DTI'),\n",
       "  ('of', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('most', 'QL'),\n",
       "  ('important', 'JJ'),\n",
       "  ('tasks', 'NNS')],\n",
       " [('with', 'IN'),\n",
       "  ('further', 'JJR'),\n",
       "  ('advances', 'NNS'),\n",
       "  ('the', 'AT'),\n",
       "  ('technology', 'NN'),\n",
       "  ('could', 'MD'),\n",
       "  ('have', 'HV'),\n",
       "  ('a', 'AT'),\n",
       "  ('profound', 'JJ'),\n",
       "  ('impact', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('medicine', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('other', 'AP'),\n",
       "  ('fields', 'NNS')],\n",
       " [('it', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('that', 'CS'),\n",
       "  ('this', 'DT'),\n",
       "  ('approach', 'NN'),\n",
       "  ('could', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('decode', 'NN'),\n",
       "  ('information', 'NN'),\n",
       "  ('about', 'IN'),\n",
       "  ('what', 'WDT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('a', 'AT'),\n",
       "  ('person', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('hearing', 'NN'),\n",
       "  ('reading', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('possibly', 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('thinking', 'VBG'),\n",
       "  ('said', 'VBD'),\n",
       "  ('alexander', 'NN'),\n",
       "  ('huth', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('first', 'OD'),\n",
       "  ('author', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN')],\n",
       " [('one', 'CD'),\n",
       "  ('potential', 'JJ'),\n",
       "  ('use', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('a', 'AT'),\n",
       "  ('language', 'NN'),\n",
       "  ('decoder', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('could', 'MD'),\n",
       "  ('allow', 'VB'),\n",
       "  ('people', 'NNS'),\n",
       "  ('silenced', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('motor', 'NN'),\n",
       "  ('neurone', 'NN'),\n",
       "  ('disease', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('locked', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('syndrome', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('speak', 'VB'),\n",
       "  ('through', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('computer', 'NN')],\n",
       " [('to', 'IN-HL'),\n",
       "  ('create', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('recorded', 'VBN'),\n",
       "  ('people', 'NNS'),\n",
       "  ('s', 'NN'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('while', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('read', 'VB'),\n",
       "  ('out', 'RP'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('moth', 'NN'),\n",
       "  ('radio', 'NN'),\n",
       "  ('hour', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('us', 'PPO'),\n",
       "  ('radio', 'NN'),\n",
       "  ('show', 'NN')],\n",
       " [('they', 'PPSS'),\n",
       "  ('then', 'RB'),\n",
       "  ('matched', 'VBN'),\n",
       "  ('the', 'AT'),\n",
       "  ('transcripts', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('data', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('show', 'NN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('groups', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('related', 'VBN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('triggered', 'VBN'),\n",
       "  ('neural', 'JJ'),\n",
       "  ('responses', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('50', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('80', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('pea', 'NN'),\n",
       "  ('sized', 'VBD'),\n",
       "  ('spots', 'NNS'),\n",
       "  ('all', 'ABN'),\n",
       "  ('over', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('cerebral', 'JJ'),\n",
       "  ('cortex', 'NN')],\n",
       " [('huth', 'NN'),\n",
       "  ('used', 'VBN'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('moth', 'NN'),\n",
       "  ('radio', 'NN'),\n",
       "  ('hour', 'NN'),\n",
       "  ('because', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('short', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('compelling', 'JJ')],\n",
       " [('the', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('enthralling', 'JJ'),\n",
       "  ('the', 'AT'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('the', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('confident', 'JJ'),\n",
       "  ('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('could', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('that', 'CS'),\n",
       "  ('the', 'AT'),\n",
       "  ('people', 'NNS'),\n",
       "  ('being', 'BEG'),\n",
       "  ('scanned', 'VBD'),\n",
       "  ('were', 'BED'),\n",
       "  ('focusing', 'VBG'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('not', '*'),\n",
       "  ('drifting', 'VBG'),\n",
       "  ('off', 'RP')],\n",
       " [('seven', 'CD'),\n",
       "  ('people', 'NNS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('two', 'CD'),\n",
       "  ('hours', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('stories', 'NNS'),\n",
       "  ('each', 'DT')],\n",
       " [('per', 'IN'),\n",
       "  ('person', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('amounted', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('hearing', 'NN'),\n",
       "  ('roughly', 'QL'),\n",
       "  ('25', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('more', 'AP'),\n",
       "  ('than', 'IN'),\n",
       "  ('3', 'CD'),\n",
       "  ('000', 'NN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('as', 'CS'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('lay', 'VBD'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('scanner', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('shows', 'VBZ'),\n",
       "  ('how', 'WRB'),\n",
       "  ('words', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('related', 'VBN'),\n",
       "  ('terms', 'NNS'),\n",
       "  ('exercise', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN')],\n",
       " [('for', 'CS'),\n",
       "  ('example', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('left', 'NR'),\n",
       "  ('hand', 'NN'),\n",
       "  ('side', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('above', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('ear', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('tiny', 'JJ'),\n",
       "  ('regions', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represents', 'VBZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('victim', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('region', 'NN'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('killed', 'VBN'),\n",
       "  ('convicted', 'VBN'),\n",
       "  ('murdered', 'VBN'),\n",
       "  ('and', 'CC'),\n",
       "  ('confessed', 'VBD')],\n",
       " [('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('right', 'QL'),\n",
       "  ('hand', 'NN'),\n",
       "  ('side', 'NN'),\n",
       "  ('near', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('top', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('head', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('spots', 'NNS'),\n",
       "  ('activated', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('family', 'NN'),\n",
       "  ('terms', 'NNS'),\n",
       "  ('wife', 'NN'),\n",
       "  ('husband', 'NN'),\n",
       "  ('children', 'NNS'),\n",
       "  ('parents', 'NNS')],\n",
       " [('each', 'DT'),\n",
       "  ('word', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('by', 'IN'),\n",
       "  ('more', 'AP'),\n",
       "  ('than', 'IN'),\n",
       "  ('one', 'CD'),\n",
       "  ('spot', 'NN'),\n",
       "  ('because', 'CS'),\n",
       "  ('words', 'NNS'),\n",
       "  ('tend', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('have', 'HV'),\n",
       "  ('several', 'AP'),\n",
       "  ('meanings', 'NNS')],\n",
       " [('one', 'CD'),\n",
       "  ('part', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('example', 'NN'),\n",
       "  ('reliably', 'NN'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('top', 'NN'),\n",
       "  ('along', 'IN'),\n",
       "  ('with', 'IN'),\n",
       "  ('other', 'AP'),\n",
       "  ('words', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('describe', 'VB'),\n",
       "  ('clothing', 'NN')],\n",
       " [('but', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('top', 'NN'),\n",
       "  ('activates', 'NN'),\n",
       "  ('many', 'AP'),\n",
       "  ('other', 'AP'),\n",
       "  ('regions', 'NNS')],\n",
       " [('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('them', 'PPO'),\n",
       "  ('responds', 'VBZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('numbers', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('measurements', 'NNS'),\n",
       "  ('another', 'DT'),\n",
       "  ('to', 'IN'),\n",
       "  ('buildings', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('places', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('created', 'VBN'),\n",
       "  ('an', 'AT'),\n",
       "  ('interactive', 'NN'),\n",
       "  ('website', 'NN'),\n",
       "  ('where', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('public', 'JJ'),\n",
       "  ('can', 'MD'),\n",
       "  ('explore', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN')],\n",
       " [('strikingly', 'RB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlases', 'NN'),\n",
       "  ('were', 'BED'),\n",
       "  ('similar', 'JJ'),\n",
       "  ('for', 'IN'),\n",
       "  ('all', 'ABN'),\n",
       "  ('the', 'AT'),\n",
       "  ('participants', 'NNS'),\n",
       "  ('suggesting', 'VBG'),\n",
       "  ('that', 'CS'),\n",
       "  ('their', 'PP$'),\n",
       "  ('brains', 'NNS'),\n",
       "  ('organised', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('words', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('way', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('scientists', 'NNS'),\n",
       "  ('only', 'RB'),\n",
       "  ('scanned', 'VBD'),\n",
       "  ('five', 'CD'),\n",
       "  ('men', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('two', 'CD'),\n",
       "  ('women', 'NNS'),\n",
       "  ('however', 'RB')],\n",
       " [('all', 'ABN'),\n",
       "  ('are', 'BER'),\n",
       "  ('native', 'JJ'),\n",
       "  ('english', 'NN'),\n",
       "  ('speakers', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('two', 'CD'),\n",
       "  ('are', 'BER'),\n",
       "  ('authors', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('study', 'NN'),\n",
       "  ('published', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('nature', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('highly', 'QL'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('that', 'CS'),\n",
       "  ('people', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('different', 'JJ'),\n",
       "  ('backgrounds', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('cultures', 'NNS'),\n",
       "  ('will', 'MD'),\n",
       "  ('have', 'HV'),\n",
       "  ('different', 'JJ'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlases', 'NN')],\n",
       " [('armed', 'VBN'),\n",
       "  ('with', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('researchers', 'NNS'),\n",
       "  ('can', 'MD'),\n",
       "  ('now', 'RB'),\n",
       "  ('piece', 'NN'),\n",
       "  ('together', 'RB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('networks', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('represent', 'VB'),\n",
       "  ('wildly', 'RB'),\n",
       "  ('different', 'JJ'),\n",
       "  ('concepts', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('numbers', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('murder', 'VB'),\n",
       "  ('and', 'CC'),\n",
       "  ('religion', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('idea', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('murder', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('a', 'AT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('said', 'VBD')],\n",
       " [('using', 'VBG'),\n",
       "  ('the', 'AT'),\n",
       "  ('same', 'AP'),\n",
       "  ('haul', 'VB'),\n",
       "  ('of', 'IN'),\n",
       "  ('data', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('group', 'NN'),\n",
       "  ('has', 'HVZ'),\n",
       "  ('begun', 'VBN'),\n",
       "  ('work', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('new', 'JJ'),\n",
       "  ('atlases', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('show', 'VB'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('holds', 'VBZ'),\n",
       "  ('information', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('other', 'AP'),\n",
       "  ('aspects', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('language', 'NN'),\n",
       "  ('from', 'IN'),\n",
       "  ('phonemes', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('syntax', 'NN')],\n",
       " [('a', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('structure', 'NN'),\n",
       "  ('has', 'HVZ'),\n",
       "  ('so', 'QL'),\n",
       "  ('far', 'RB'),\n",
       "  ('proved', 'VBD'),\n",
       "  ('elusive', 'JJ'),\n",
       "  ('however', 'WRB')],\n",
       " [('every', 'AT'),\n",
       "  ('time', 'NN'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('come', 'VB'),\n",
       "  ('up', 'RP'),\n",
       "  ('with', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('set', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('features', 'NNS'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('get', 'VB'),\n",
       "  ('told', 'VBD'),\n",
       "  ('they', 'PPSS'),\n",
       "  ('aren', 'NN'),\n",
       "  ('t', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('right', 'NN'),\n",
       "  ('set', 'VBN'),\n",
       "  ('of', 'IN'),\n",
       "  ('narrative', 'NN'),\n",
       "  ('features', 'NNS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('gallant', 'JJ')],\n",
       " [('uri', 'NN'),\n",
       "  ('hasson', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('princeton', 'NN'),\n",
       "  ('university', 'NN'),\n",
       "  ('praised', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('work', 'NN')],\n",
       " [('unlike', 'IN'),\n",
       "  ('many', 'AP'),\n",
       "  ('studies', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('looked', 'VBD'),\n",
       "  ('at', 'IN'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('when', 'WRB'),\n",
       "  ('an', 'AT'),\n",
       "  ('isolated', 'VBN'),\n",
       "  ('word', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('sentence', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('spoken', 'VBN'),\n",
       "  ('gallant', 'JJ'),\n",
       "  ('s', 'NN'),\n",
       "  ('team', 'NN'),\n",
       "  ('had', 'HVD'),\n",
       "  ('shed', 'NN'),\n",
       "  ('light', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('worked', 'VBD'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('real', 'JJ'),\n",
       "  ('world', 'NN'),\n",
       "  ('scenario', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('the', 'AT'),\n",
       "  ('next', 'AP'),\n",
       "  ('step', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('to', 'TO'),\n",
       "  ('create', 'VB'),\n",
       "  ('a', 'AT'),\n",
       "  ('more', 'QL'),\n",
       "  ('comprehensive', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('precise', 'JJ'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN')],\n",
       " [('ultimately', 'RB'),\n",
       "  ('hasson', 'NN'),\n",
       "  ('believes', 'VBZ'),\n",
       "  ('it', 'PPO'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('reconstruct', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('words', 'NNS'),\n",
       "  ('a', 'AT'),\n",
       "  ('person', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('thinking', 'VBG'),\n",
       "  ('from', 'IN'),\n",
       "  ('their', 'PP$'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('ethical', 'JJ'),\n",
       "  ('implications', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('enormous', 'JJ')],\n",
       " [('one', 'CD'),\n",
       "  ('more', 'AP'),\n",
       "  ('benign', 'JJ'),\n",
       "  ('use', 'NN'),\n",
       "  ('would', 'MD'),\n",
       "  ('see', 'VB'),\n",
       "  ('brain', 'NN'),\n",
       "  ('activity', 'NN'),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('assess', 'VB'),\n",
       "  ('whether', 'CS'),\n",
       "  ('political', 'JJ'),\n",
       "  ('messages', 'NNS'),\n",
       "  ('have', 'HV'),\n",
       "  ('been', 'BEN'),\n",
       "  ('effectively', 'RB'),\n",
       "  ('communicated', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'AT'),\n",
       "  ('public', 'JJ')],\n",
       " [('there', 'EX'),\n",
       "  ('are', 'BER'),\n",
       "  ('so', 'QL'),\n",
       "  ('many', 'AP'),\n",
       "  ('implications', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('we', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('barely', 'RB'),\n",
       "  ('touching', 'JJ'),\n",
       "  ('the', 'AT'),\n",
       "  ('surface', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('lorraine', 'NN'),\n",
       "  ('tyler', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('cognitive', 'JJ'),\n",
       "  ('neuroscientist', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('head', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('centre', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('speech', 'NN'),\n",
       "  ('language', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('cambridge', 'NN'),\n",
       "  ('university', 'NN'),\n",
       "  ('said', 'VBD'),\n",
       "  ('the', 'AT'),\n",
       "  ('research', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('tour', 'NN'),\n",
       "  ('de', 'FW-IN'),\n",
       "  ('force', 'FW-NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('scope', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('methods', 'NNS')],\n",
       " [('but', 'CC'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN'),\n",
       "  ('atlas', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('current', 'JJ'),\n",
       "  ('form', 'NN'),\n",
       "  ('does', 'DOZ'),\n",
       "  ('not', '*'),\n",
       "  ('capture', 'VB'),\n",
       "  ('fine', 'JJ'),\n",
       "  ('differences', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('word', 'NN'),\n",
       "  ('meanings', 'NNS'),\n",
       "  ('take', 'VB'),\n",
       "  ('the', 'AT'),\n",
       "  ('word', 'NN'),\n",
       "  ('table', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('a', 'AT'),\n",
       "  ('member', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('many', 'AP'),\n",
       "  ('different', 'JJ'),\n",
       "  ('groups', 'NNS'),\n",
       "  ('says', 'VBZ'),\n",
       "  ('tyler', 'NN')],\n",
       " [('it', 'PPS'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'BE'),\n",
       "  ('something', 'PN'),\n",
       "  ('to', 'TO'),\n",
       "  ('eat', 'VB'),\n",
       "  ('off', 'RP'),\n",
       "  ('things', 'NNS'),\n",
       "  ('made', 'VBN'),\n",
       "  ('of', 'IN'),\n",
       "  ('wood', 'NN'),\n",
       "  ('things', 'NNS'),\n",
       "  ('that', 'WPS'),\n",
       "  ('are', 'BER'),\n",
       "  ('heavy', 'JJ'),\n",
       "  ('things', 'NNS'),\n",
       "  ('having', 'HVG'),\n",
       "  ('four', 'CD'),\n",
       "  ('legs', 'NNS'),\n",
       "  ('non', 'FW-*'),\n",
       "  ('animate', 'JJ'),\n",
       "  ('objects', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('so', 'RB'),\n",
       "  ('on', 'IN')],\n",
       " [('this', 'DT'),\n",
       "  ('kind', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('detailed', 'VBN'),\n",
       "  ('semantic', 'JJ'),\n",
       "  ('information', 'NN'),\n",
       "  ('that', 'CS'),\n",
       "  ('enables', 'VBZ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('be', 'BE'),\n",
       "  ('used', 'VBN'),\n",
       "  ('flexibly', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('lost', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('she', 'PPS'),\n",
       "  ('said', 'VBD')],\n",
       " [('while', 'CS'),\n",
       "  ('this', 'DT'),\n",
       "  ('research', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('path', 'NN'),\n",
       "  ('breaking', 'VBG'),\n",
       "  ('in', 'IN'),\n",
       "  ('its', 'PP$'),\n",
       "  ('scope', 'NN'),\n",
       "  ('there', 'EX'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('still', 'RB'),\n",
       "  ('a', 'AT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('learn', 'VB'),\n",
       "  ('about', 'IN'),\n",
       "  ('how', 'WRB'),\n",
       "  ('semantics', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('represented', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('brain', 'NN')]]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sciencearticle_line.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "#         filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list])\n",
    "        testmeta1.append(tokenizer.tokenize(filtered_words))\n",
    "testmeta1[0]\n",
    "\n",
    "def get_pos(sents, tagger):        \n",
    "    return [tagger.tag(sent) for sent in sents]\n",
    "\n",
    "pos = get_pos(testmeta1, brown_tagger)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "def train_tagger_on_brown_augmented_with_additional_sents():\n",
    "\n",
    "    additional_sents = [[('colorful', 'JJ'), ('quilt', 'NN')],\n",
    "                        [('regions', 'NN'), ('represent', 'VB'), ('world', 'NN')],\n",
    "                        [('public', 'NN'), ('explore', 'VB'), ('brain', 'NN')]]\n",
    "\n",
    "\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "    \n",
    "    #append hand-tagged cooking sentences to the front of the training data\n",
    "    all_tagged_sents = additional_sents + brown_tagged_sents\n",
    "    return train_tagger(all_tagged_sents)\n",
    "\n",
    "brown_and_additional_tagger = train_tagger_on_brown_augmented_with_cooking_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "better_sentences = get_pos(testmeta1, brown_and_additional_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 'AT'),\n",
       "  ('snow', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('white', 'JJ'),\n",
       "  ('blanket', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('hospital', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('refrigerator', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('classroom', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('zoo', 'NN')],\n",
       " [('america', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('melting', 'VBG'),\n",
       "  ('pot', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('lovely', 'JJ'),\n",
       "  ('voice', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('music', 'NN'),\n",
       "  ('to', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('ears', 'NNS')],\n",
       " [('life', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('roller', 'NN'),\n",
       "  ('coaster', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('alligator', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('teeth', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('white', 'JJ'),\n",
       "  ('daggers', 'NN')],\n",
       " [('their', 'PP$'),\n",
       "  ('home', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('prison', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('slide', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('playground', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('hot', 'JJ'),\n",
       "  ('stove', 'NN')],\n",
       " [('his', 'PP$'),\n",
       "  ('heart', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('iron', 'NN')],\n",
       " [('she', 'PPS'), ('is', 'BEZ'), ('a', 'AT'), ('peacock', 'NN')],\n",
       " [('he', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('shining', 'VBG'),\n",
       "  ('star', 'NN')],\n",
       " [('time', 'NN'), ('is', 'BEZ'), ('money', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('teacher', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('dragon', 'NN')],\n",
       " [('tom', 'NN'), ('s', 'NN'), ('eyes', 'NNS'), ('were', 'BED'), ('ice', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('detective', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('face', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('wood', 'NN'),\n",
       "  ('as', 'CS'),\n",
       "  ('he', 'PPS'),\n",
       "  ('listened', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('her', 'PPO'),\n",
       "  ('story', 'NN')],\n",
       " [('she', 'PPS'),\n",
       "  ('feels', 'VBZ'),\n",
       "  ('that', 'CS'),\n",
       "  ('life', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('fashion', 'NN'),\n",
       "  ('show', 'NN')],\n",
       " [('the', 'AT'), ('world', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('stage', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('kid', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('room', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('disaster', 'NN'),\n",
       "  ('area', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('children', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('flowers', 'NNS'),\n",
       "  ('grown', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('concrete', 'NN'),\n",
       "  ('gardens', 'NNS')],\n",
       " [('kisses', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('the', 'AT'),\n",
       "  ('flowers', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('affection', 'NN')],\n",
       " [('his', 'PP$'),\n",
       "  ('words', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('cotton', 'NN'),\n",
       "  ('candy', 'NN')],\n",
       " [('mary', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('eyes', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('fireflies', 'NN')],\n",
       " [('john', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('suggestion', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('just', 'RB'),\n",
       "  ('a', 'AT'),\n",
       "  ('band', 'NN'),\n",
       "  ('aid', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('cast', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('broken', 'VBN'),\n",
       "  ('leg', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('plaster', 'NN'),\n",
       "  ('shackle', 'NN')],\n",
       " [('jane', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('ambitions', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('a', 'AT'),\n",
       "  ('house', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('cards', 'NNS')],\n",
       " [('her', 'PP$'),\n",
       "  ('long', 'JJ'),\n",
       "  ('hair', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('flowing', 'VBG'),\n",
       "  ('golden', 'JJ'),\n",
       "  ('river', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('computers', 'NNS'),\n",
       "  ('at', 'IN'),\n",
       "  ('school', 'NN'),\n",
       "  ('are', 'BER'),\n",
       "  ('old', 'JJ'),\n",
       "  ('dinosaurs', 'NNS')],\n",
       " [('laughter', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('music', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('soul', 'NN')],\n",
       " [('he', 'PPS'), ('is', 'BEZ'), ('a', 'AT'), ('night', 'NN'), ('owl', 'NN')],\n",
       " [('maria', 'FW-NNS'), ('is', 'BEZ'), ('a', 'AT'), ('chicken', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('falling', 'VBG'),\n",
       "  ('snowflakes', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('dancers', 'NNS')],\n",
       " [('with', 'IN'),\n",
       "  ('his', 'PP$'),\n",
       "  ('new', 'JJ'),\n",
       "  ('haircut', 'NN'),\n",
       "  ('he', 'PPS'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('sheepdog', 'NN')],\n",
       " [('at', 'IN'),\n",
       "  ('five', 'CD'),\n",
       "  ('o', 'NN'),\n",
       "  ('clock', 'NN'),\n",
       "  ('the', 'AT'),\n",
       "  ('interstate', 'JJ'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('parking', 'VBG'),\n",
       "  ('lot', 'NN')],\n",
       " [('books', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('keys', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('your', 'PP$'),\n",
       "  ('imagination', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('teddy', 'NN'),\n",
       "  ('bear', 'VB'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('her', 'PP$'),\n",
       "  ('best', 'JJT'),\n",
       "  ('friend', 'NN'),\n",
       "  ('never', 'RB'),\n",
       "  ('telling', 'VBG'),\n",
       "  ('her', 'PP$'),\n",
       "  ('secrets', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('peaceful', 'JJ'),\n",
       "  ('lake', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('mirror', 'NN')],\n",
       " [('terry', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('blue', 'JJ'),\n",
       "  ('when', 'WRB'),\n",
       "  ('his', 'PP$'),\n",
       "  ('goldfish', 'NN'),\n",
       "  ('died', 'VBD')],\n",
       " [('the', 'AT'),\n",
       "  ('wind', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('an', 'AT'),\n",
       "  ('angry', 'JJ'),\n",
       "  ('witch', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('ballerina', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('swan', 'NN'),\n",
       "  ('gliding', 'NN'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('stage', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('angry', 'JJ'),\n",
       "  ('words', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('bullets', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('him', 'PPO')],\n",
       " [('your', 'PP$'),\n",
       "  ('brain', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('computer', 'NN')],\n",
       " [('jamal', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('pig', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('dinner', 'NN')],\n",
       " [('you', 'PPSS'), ('are', 'BER'), ('my', 'PP$'), ('sunshine', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('car', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('furnace', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('sun', 'NN')],\n",
       " [('thank', 'VB'),\n",
       "  ('you', 'PPO'),\n",
       "  ('so', 'QL'),\n",
       "  ('much', 'AP'),\n",
       "  ('you', 'PPSS'),\n",
       "  ('are', 'BER'),\n",
       "  ('an', 'AT'),\n",
       "  ('angel', 'NN')],\n",
       " [('that', 'DT'),\n",
       "  ('coach', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('an', 'AT'),\n",
       "  ('ogre', 'NN')],\n",
       " [('ben', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('temper', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('volcano', 'NN'),\n",
       "  ('ready', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('explode', 'VB')],\n",
       " [('the', 'AT'),\n",
       "  ('kids', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('monkeys', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('jungle', 'NN'),\n",
       "  ('gym', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('sun', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('golden', 'JJ'),\n",
       "  ('ball', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('clouds', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('balls', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('cotton', 'NN')],\n",
       " [('sue', 'VB'),\n",
       "  ('s', 'NN'),\n",
       "  ('room', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('zoo', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('fish', 'NN'),\n",
       "  ('a', 'AT'),\n",
       "  ('gerbil', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('a', 'AT'),\n",
       "  ('parakeet', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('park', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('lake', 'NN'),\n",
       "  ('after', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('rain', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('lightning', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('fireworks', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('sky', 'NN')],\n",
       " [('gary', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('mule', 'NN')],\n",
       " [('that', 'DT'),\n",
       "  ('lawn', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('green', 'JJ'),\n",
       "  ('carpet', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('dad', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('road', 'NN'),\n",
       "  ('hog', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('stars', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('sparkling', 'VBG'),\n",
       "  ('diamonds', 'NNS')],\n",
       " [('those', 'DTS'),\n",
       "  ('two', 'CD'),\n",
       "  ('best', 'JJT'),\n",
       "  ('friends', 'NNS'),\n",
       "  ('are', 'BER'),\n",
       "  ('two', 'CD'),\n",
       "  ('peas', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('pod', 'NN')],\n",
       " [('he', 'PPS'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('walking', 'VBG'),\n",
       "  ('dictionary', 'NN')],\n",
       " [('donations', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('popular', 'JJ'),\n",
       "  ('charity', 'NN'),\n",
       "  ('were', 'BED'),\n",
       "  ('a', 'AT'),\n",
       "  ('tsunami', 'NN')],\n",
       " [('necessity', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('the', 'AT'),\n",
       "  ('mother', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('invention', 'NN')],\n",
       " [('my', 'PP$'),\n",
       "  ('big', 'JJ'),\n",
       "  ('brother', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('couch', 'NN'),\n",
       "  ('potato', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('road', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('ribbon', 'NN'),\n",
       "  ('stretching', 'VBG'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('desert', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('teenager', 'NN'),\n",
       "  ('s', 'NN'),\n",
       "  ('stomach', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('bottomless', 'JJ'),\n",
       "  ('pit', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('thunder', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('mighty', 'JJ'),\n",
       "  ('lion', 'NN')],\n",
       " [('i', 'NN'),\n",
       "  ('am', 'BEM'),\n",
       "  ('so', 'QL'),\n",
       "  ('excited', 'VBN'),\n",
       "  ('my', 'PP$'),\n",
       "  ('pulse', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('race', 'NN'),\n",
       "  ('car', 'NN')],\n",
       " [('the', 'AT'),\n",
       "  ('moon', 'NN'),\n",
       "  ('is', 'BEZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('white', 'JJ'),\n",
       "  ('balloon', 'NN')],\n",
       " [('toddlers', 'NNS'), ('are', 'BER'), ('rug', 'NN'), ('rats', 'NNS')],\n",
       " [('the', 'AT'),\n",
       "  ('stormy', 'JJ'),\n",
       "  ('ocean', 'NN'),\n",
       "  ('was', 'BEDZ'),\n",
       "  ('a', 'AT'),\n",
       "  ('raging', 'VBG'),\n",
       "  ('bull', 'NN')],\n",
       " [('her', 'PP$'),\n",
       "  ('tears', 'NNS'),\n",
       "  ('were', 'BED'),\n",
       "  ('a', 'AT'),\n",
       "  ('river', 'NN'),\n",
       "  ('flowing', 'VBG'),\n",
       "  ('down', 'RP'),\n",
       "  ('her', 'PP$'),\n",
       "  ('cheeks', 'NNS')]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"test_meta_sent.txt\", \"r\") as testset:\n",
    "    filtered_words = []\n",
    "    testmeta = []\n",
    "    testmeta1 = []\n",
    "    word_list = ''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for line in testset:\n",
    "        word_list = line.split()\n",
    "#         filtered_words = ' '.join([word.lower() for word in word_list if word not in stopwords.words('english')])\n",
    "        filtered_words = ' '.join([word.lower() for word in word_list])\n",
    "        testmeta1.append(tokenizer.tokenize(filtered_words))\n",
    "testmeta1[0]\n",
    "\n",
    "def get_pos(sents, tagger):        \n",
    "    return [tagger.tag(sent) for sent in sents]\n",
    "\n",
    "pos = get_pos(testmeta1, brown_tagger)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

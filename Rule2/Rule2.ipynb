{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule 2: Never use a long word where a short word will do\n",
    "\n",
    "We are going to take a loose interpretation of this. Instead of word length, we will use both the number of syllables and the order of where it appears in order in a frequency distribution of words. If we just did number of syllables, we would, for example, always replace the word `therefore` with `thus`, which is not in the spirit of the problem. \n",
    "\n",
    "So let's get cracking on this score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import pprint as pp\n",
    "import re\n",
    "import spacy\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from google_ngram_downloader import readline_google_store\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "syllable_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code was directly copied from Rule #5\n",
    "# We use it to minimize the words we need to check for because it's a computationally heavy task\n",
    "\n",
    "def strip_non_words(tokenized_text):\n",
    "    return [token for token in tokenized_text if token.is_alpha==True]\n",
    "\n",
    "# Takes in a bag of words and spits out that same bag of words but without the proper nouns\n",
    "def strip_proper_nouns(tokenized_text):\n",
    "    return [token for token in tokenized_text if token.tag_ != 'NNP' and token.tag_ != 'NNPS']\n",
    "\n",
    "# Takes in a bag of words and removes any of them that are in the top n most common words\n",
    "def strip_most_common_words(tokenized_text, n_most_common=10000):\n",
    "    # Build the list of most common words\n",
    "    most_common_words = []\n",
    "    google_most_common_words_path = sys.path[1] + '/../Texts/google-10000-english-usa.txt'\n",
    "    with open(google_most_common_words_path, 'r') as f:\n",
    "        for i in range(n_most_common):\n",
    "            most_common_words.append(f.readline().strip())\n",
    "    # Remove anything in the n most common words\n",
    "    return [token for token in tokenized_text if token.text.lower() not in most_common_words]\n",
    "\n",
    "def strip_non_jargon_words(tokenized_text):\n",
    "    text_no_proper_nouns = strip_proper_nouns(tokenized_text)\n",
    "    text_no_non_words = strip_non_words(text_no_proper_nouns)\n",
    "    text_no_common_words = strip_most_common_words(text_no_non_words)\n",
    "    return text_no_common_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This number comes from Google's blog\n",
    "# https://research.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html\n",
    "# TODO: If there's time, confirm this number\n",
    "NGRAM_TOKEN_COUNT = 1024908267229\n",
    "\n",
    "# Shout out to Quora for this snippet of code\n",
    "# https://www.quora.com/Is-there-any-Google-Ngram-API-for-Python\n",
    "\n",
    "def find_google_ngrams_word_count(word, time_function=False, verbose=False):\n",
    "    if time_function == True:\n",
    "        time1 = time.time()\n",
    "\n",
    "    count = 2 # Set this to a minimum of 2 so we don't get a divide by zero error\n",
    "    # TODO: Consider how we want to deal with capitalization\n",
    "    fname, url, records = next(readline_google_store(ngram_len=1, indices=word[0]))\n",
    "    # If we use the verbose settings, occaisionally print out the record\n",
    "    verbosity_count = 1000000000\n",
    "    earliest_year = 1950\n",
    "    i = 0\n",
    "    try:\n",
    "        record = next(records)\n",
    "        while record.ngram != word:\n",
    "            record = next(records)\n",
    "            if verbose == True and i%verbosity_count == 0:\n",
    "                print(record)\n",
    "            i += 1\n",
    "        while record.ngram == word:\n",
    "            if record.year >= earliest_year:\n",
    "                count += record.match_count\n",
    "                if verbose == True:\n",
    "                    print(record)\n",
    "            record = next(records)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    # Default to 1 so our program doesn't crash\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    if time_function == True:\n",
    "        time2 = time.time()\n",
    "    print('Total seconds for ' + word + ': ' + str(int((time2-time1))))\n",
    "    return count\n",
    "\n",
    "def find_frequency_score(word):\n",
    "    unigram_count = find_google_ngrams_word_count(word, time_function=True)\n",
    "    percent_occurrence = unigram_count/NGRAM_TOKEN_COUNT\n",
    "    # Get the log of the frequency to make our number manageable\n",
    "    freq_val = math.log(percent_occurrence)\n",
    "    max_ngram_val = math.log(1/NGRAM_TOKEN_COUNT)\n",
    "    relative_freq = ((freq_val - max_ngram_val)/(-max_ngram_val))\n",
    "    return round(relative_freq, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n",
      "[wore, wore, soliloquy, bifocals]\n",
      "[1, 1, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "BIG_NUMBER = 18109831\n",
    "\n",
    "def syllable_count(word, syllable_dict):\n",
    "    syllable_count = 0\n",
    "    for word in word.split():\n",
    "        if word in syllable_dict:\n",
    "            # Shout out to StackOverflow for this snippet of code\n",
    "            # http://stackoverflow.com/a/4103234/1031615\n",
    "            syllable_count += [len(list(y for y in x if y[-1].isdigit())) for x in syllable_dict[word]][0]\n",
    "            continue\n",
    "        # If it's not in the dictionary count the number of vowels and ignore an e at the end not\n",
    "        # preceded by another vowel. It's rough, but there will be few cases if any cases in which\n",
    "        # a word is not in the CMU dictionary but in WordNet\n",
    "        if word[-1] == 'e':\n",
    "            word = word[:-1]\n",
    "        word = re.sub(r'[^aeiou]', '', word)\n",
    "        syllable_count += len(word)\n",
    "    return max(syllable_count, 1)\n",
    "\n",
    "\n",
    "def readability_for_word(word, syllable_dict, use_ngrams = False):\n",
    "    if word is None:\n",
    "        return BIG_NUMBER \n",
    "    syllables = syllable_count(word.lower(), syllable_dict)\n",
    "    if use_ngrams == False:\n",
    "        return syllables\n",
    "    freq_score = find_frequency_score(word.lower())\n",
    "    return syllables * freq_score\n",
    "   \n",
    "    \n",
    "example_text = \"Sometimes she wore glasses. Other times, she wore soliloquy bifocals. \"\n",
    "tokenized_text = nlp(example_text)\n",
    "print(tokenized_text[2].pos_)\n",
    "# Strip out common words and proper nouns for optimization's sake\n",
    "stripped_text = strip_non_jargon_words(tokenized_text)\n",
    "pp.pprint(stripped_text)\n",
    "scored_text = [readability_for_word(token.text.lower(), syllable_dict) for token in stripped_text if token.is_alpha]\n",
    "pp.pprint(scored_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Now let's bust some synsets up in here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optical instrument not in syllable dict\n",
      ". not in syllable dict\n",
      "time period not in syllable dict\n",
      ", not in syllable dict\n",
      "actor's line not in syllable dict\n",
      "4\n",
      "actor's line not in syllable dict\n",
      "3\n",
      ". not in syllable dict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Sometimes', None),\n",
       " ('she', None),\n",
       " ('wore', None),\n",
       " ('glasses', None),\n",
       " ('.', None),\n",
       " ('Other', None),\n",
       " ('times', None),\n",
       " (',', None),\n",
       " ('she', None),\n",
       " ('wore', None),\n",
       " ('soliloquy', \"actor's line\"),\n",
       " ('bifocals', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def synsets_for_tokens_in_tokenized_sentence(tokenized_sentence):\n",
    "    sentence = [token.text for token in tokenized_sentence]\n",
    "    synsets = [lesk(sentence, token.text, spacy_to_wordnet_pos(token.pos_)) for token in tokenized_sentence]\n",
    "    for i in range(len(synsets)):\n",
    "        # Get the hypernym of the word\n",
    "        if synsets[i] is not None:\n",
    "            if len(synsets[i].hypernyms()) is not 0:\n",
    "                synsets[i] = synsets[i].hypernyms()[0]\n",
    "            synsets[i] = synsets[i].name().split('.')[0].replace('_', ' ')\n",
    "    return synsets\n",
    "\n",
    "def spacy_to_wordnet_pos(pos):\n",
    "    # To see all the parts of speech spaCy uses, see the link below\n",
    "    # http://polyglot.readthedocs.io/en/latest/POS.html\n",
    "    if pos == 'ADJ':\n",
    "        return wn.ADJ\n",
    "    elif pos == 'ADV':\n",
    "        return wn.ADV\n",
    "    elif pos == 'NOUN':\n",
    "        return wn.NOUN\n",
    "    elif pos == 'VERB':\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "# Returns an array of tuples. If the word cannot be replaced, the second value is the replacing word.\n",
    "# If it cannot be replaced, it is None\n",
    "def replaceable_word_in_tokenized_sentence(tokenized_sentence):\n",
    "    sentence_words = [token.text for token in tokenized_sentence]\n",
    "    sentence_alternatives = synsets_for_tokens_in_tokenized_sentence(tokenized_sentence)\n",
    "    words_and_alternatives = zip(sentence_words, sentence_alternatives)\n",
    "    replaceable_words = []\n",
    "    for (word, alt) in words_and_alternatives:\n",
    "        if readability_for_word(word, syllable_dict) > readability_for_word(alt, syllable_dict):\n",
    "            replaceable_words.append((word, alt))\n",
    "        else:\n",
    "            replaceable_words.append((word, None))\n",
    "    return replaceable_words\n",
    "\n",
    "replaceable_word_in_tokenized_sentence(tokenized_text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule 2: Never use a long word where a short word will do\n",
    "\n",
    "We are going to take a loose interpretation of this. Instead of word length, we will use both the number of syllables and the order of where it appears in order in a frequency distribution of words. If we just did number of syllables, we would, for example, always replace the word `therefore` with `thus`, which is not in the spirit of the problem. \n",
    "\n",
    "So let's get cracking on this score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import pprint as pp\n",
    "import spacy\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from google_ngram_downloader import readline_google_store\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "syllable_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code was directly copied from Rule #5\n",
    "# We use it to minimize the words we need to check for because it's a computationally heavy task\n",
    "\n",
    "def strip_non_words(tokenized_text):\n",
    "    return [token for token in tokenized_text if token.is_alpha==True]\n",
    "\n",
    "# Takes in a bag of words and spits out that same bag of words but without the proper nouns\n",
    "def strip_proper_nouns(tokenized_text):\n",
    "    return [token for token in tokenized_text if token.tag_ != 'NNP' and token.tag_ != 'NNPS']\n",
    "\n",
    "# Takes in a bag of words and removes any of them that are in the top n most common words\n",
    "def strip_most_common_words(tokenized_text, n_most_common=10000):\n",
    "    # Build the list of most common words\n",
    "    most_common_words = []\n",
    "    google_most_common_words_path = sys.path[1] + '/../Texts/google-10000-english-usa.txt'\n",
    "    with open(google_most_common_words_path, 'r') as f:\n",
    "        for i in range(n_most_common):\n",
    "            most_common_words.append(f.readline().strip())\n",
    "    # Remove anything in the n most common words\n",
    "    return [token for token in tokenized_text if token.text.lower() not in most_common_words]\n",
    "\n",
    "def strip_non_jargon_words(tokenized_text):\n",
    "    text_no_proper_nouns = strip_proper_nouns(tokenized_text)\n",
    "    text_no_non_words = strip_non_words(text_no_proper_nouns)\n",
    "    text_no_common_words = strip_most_common_words(text_no_non_words)\n",
    "    return text_no_common_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This number comes from Google's blog\n",
    "# https://research.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html\n",
    "\n",
    "# TODO: If there's time, confirm this number\n",
    "NGRAM_TOKEN_COUNT = 1024908267229\n",
    "\n",
    "# Shout out to Quora for this snippet of code\n",
    "# https://www.quora.com/Is-there-any-Google-Ngram-API-for-Python\n",
    "\n",
    "def find_google_ngrams_word_count(word, time_function=False, verbose=True):\n",
    "    if time_function == True:\n",
    "        time1 = time.time()\n",
    "\n",
    "    count = 2 # Set this to a minimum of 2 so we don't get a divide by zero error\n",
    "    # TODO: Consider how we want to deal with capitalization\n",
    "    fname, url, records = next(readline_google_store(ngram_len=1, indices=word[0]))\n",
    "    # If we use the verbose settings, occaisionally print out the record\n",
    "    verbosity_count = 10000000\n",
    "    i = 0\n",
    "    try:\n",
    "        record = next(records)\n",
    "        while record.ngram != word:\n",
    "            record = next(records)\n",
    "            if verbose == True and i%verbosity_count == 0:\n",
    "                print(record)\n",
    "            i += 1\n",
    "        while record.ngram == word:\n",
    "            count += record.match_count\n",
    "            if verbose == True:\n",
    "                print(record)\n",
    "            record = next(records)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    # Default to 1 so our program doesn't crash\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    if time_function == True:\n",
    "        time2 = time.time()\n",
    "    print('Total seconds: ' + str(int((time2-time1))))\n",
    "    return count\n",
    "\n",
    "def find_frequency_score(word):\n",
    "    unigram_count = find_google_ngrams_word_count(word, time_function=True)\n",
    "    percent_occurrence = unigram_count/NGRAM_TOKEN_COUNT\n",
    "    # Get the log of the frequency to make our number manageable\n",
    "    freq_val = math.log(percent_occurrence)\n",
    "    max_ngram_val = math.log(1/NGRAM_TOKEN_COUNT)\n",
    "    relative_freq = ((freq_val - max_ngram_val)/(-max_ngram_val))\n",
    "    return round(relative_freq, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wears, wears, spectacles]\n",
      "Record(ngram=\"W'ald_NOUN\", year=1899, match_count=1, volume_count=1)\n",
      "Record(ngram='Wandsbek_NOUN', year=1983, match_count=28, volume_count=14)\n"
     ]
    }
   ],
   "source": [
    "def syllable_count(word, syllable_dict):\n",
    "    if word in syllable_dict:\n",
    "        return len(syllable_dict[word])\n",
    "    # TODO\n",
    "    # If it's not in the dictionary count the number of vowels and ignore an e at the end not\n",
    "    # preceded by another vowel. It's rough, but there will be few cases if any cases in which\n",
    "    # a word is not in the CMU dictionary but in WordNet\n",
    "    return 1\n",
    "\n",
    "def readability_for_word(word, syllable_dict):\n",
    "    syllables = syllable_count(word, syllable_dict)\n",
    "    freq_score = find_frequency_score(word)\n",
    "    return (word, syllables, freq_score)\n",
    "    \n",
    "example_text = \"Sometimes she wears glasses. Other times, she wears spectacles. \"\n",
    "tokenized_text = nlp(example_text)\n",
    "# Strip out common words and proper nouns for optimization's sake\n",
    "stripped_text = strip_non_jargon_words(tokenized_text)\n",
    "pp.pprint(stripped_text)\n",
    "scored_text = [readability_for_word(token.text.lower(), syllable_dict) for token in stripped_text if token.is_alpha]\n",
    "pp.pprint(scored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WARNING: THIS CODE TAKES FOREVER TO RUN\n",
    "# import string\n",
    "\n",
    "# a_through_z = string.ascii_lowercase\n",
    "# unigram_total_match_count = 0\n",
    "# fname, url, records = next(readline_google_store(ngram_len=1, indices=a_through_z))\n",
    "\n",
    "# i = 0\n",
    "# try:\n",
    "#     while True:\n",
    "#         record = next(records)\n",
    "#         count += record.match_count\n",
    "#         if i%10000000 == 0:\n",
    "#             print(record)\n",
    "#             print('Currently at: ' + str(count))\n",
    "#         i += 1\n",
    "# except StopIteration:\n",
    "#     pass\n",
    "\n",
    "# print('Count is ' + str(count))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

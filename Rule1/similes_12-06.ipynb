{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the algorithm:**  \n",
    "take a big corpus with trite similes (Pulp Fiction collection, War and Peace, Romance books, [The Daily Mail](http://cs.nyu.edu/~kcho/DMQA/)), prepare clean sentences, traverce throug the corpus sen by sen, POS-tagging each and looking for sentences containing prepositions \"like\" and \"as-as\" (tag = \"IN\") and exclude \"as soon as, as well as, as usual, such as, as of yet, as much, like that, like this..\" (alternatively, use dependency parser to accurately cut out a phrase. but it's a pain and may be an overkill). Add these sentences to a target corpus. Cut out a simile candidate (segment of a sentence) out of each sentence; replace \"likes\" and \"ases\" with a \"comparator\".  \n",
    "Take a list of candidates and compare each of them to the rest of them. If 3 of 4 words match (75%), count this match as 1. All similes that match with other 5 similes are considered to be trite (5 is subjective; can put a higher num to get *really* trite similes; generally, the bigger the training set, the higher might be the number). Write all trite similes into a pkl-file. This is our collection. With a testing set, repeat all steps up to fuzzy matching. Then, instead of fuzzy matching candidates across the testing set, fuzzy match them with the trite similes collection. Output sentences that have a simile. For sents with trite simile - 'True', for sents with non-trite simile - 'False'.  \n",
    "**important: instal tqdm (pip tqdm install)** it will show the progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "min_simile_freq = 5\n",
    "train_dir_name = 'C:/Users/Nat/Documents/_ANLP/Final_Project/training_data/' #REASSIGN\n",
    "test_dir_name = 'C:/Users/Nat/Documents/_ANLP/Final_Project/pulitzer_testing_data/not_annotated' #REASSIGN\n",
    "\n",
    "\n",
    "# from nltk.parse.stanford import StanfordDependencyParser\n",
    "# path_to_jar = '/Development/Projects/Magnifis/3rd_party/NLU/stanford-corenlp-full-2013/stanford-corenlp-3.2.0.jar'\n",
    "# path_to_models_jar ='/Development/Projects/Magnifis/3rd_party/NLU/stanford-corenlp-full-2013/stanford-corenlp-3.2.0-models.jar'\n",
    "# dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "# result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "# dep = result.next()\n",
    "# list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "\n",
    "regex_filter = r\"(as soon)|(as well)|(as if)|(as \\w+ as possible)|(as before)|\\\n",
    "(as long)|(as usual)|(as ever)|(as a result)|\\\n",
    "(such as)|(as of yet)|(as much)|(as many)|\\\n",
    "(like that)|(like this)|(like you)|(like me)|(like him)|(like us)|(like her)|\\\n",
    "(look like)|(looks like)|\\\n",
    "(like everything else)|(like everyone else)|(anybody like)|(anyone like)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm #visualization of the processing\n",
    "\n",
    "def get_raw_text_data(input_dir):  \n",
    "    fList=os.listdir(input_dir)\n",
    "    # Create a list of file names with full path, eliminating subdirectory entries\n",
    "    fList1 = [os.path.join(input_dir, f) for f in fList if os.path.isfile(os.path.join(input_dir, f))] \n",
    "    \n",
    "    #max_files = 1000 #remove to get the entire corpus\n",
    "    raw_corpus = ''\n",
    "    for file in tqdm(fList1): #[0:max_files] \n",
    "        with codecs.open(file, 'r', 'utf-8') as f: \n",
    "                                        # 'latin_1') as f:\n",
    "        #with open(file, encoding=\"utf8\") as f:\n",
    "            raw_corpus += ''.join(f.read())  \n",
    "    corpus = re.sub(r\"(\\n|\\r)+\"\"|(@\\w+)+\", ' ', raw_corpus) #remove backslashes and words starting with @\n",
    "    #corpus = re.sub(r\"(as soon)+\" \"|(as well)+\" \"|(as if)+\" \"|(as quickly as possible)+\" \"|(as long)\" \"|(as usual)+\" \"|(such as)+\" \"|(as of yet)+\" \"|(as much)+\" \"|(as many)+\" \"|(like that)+\" \"|(like this)+\" \"|(like you)+\" \"|(like me)+\" \"|(like him)+\" \"|(like us)+\" \"|(like her)+\" \"|(anybody like)+\" \"|(anyone like)+\", \"\", corpus)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus, do_tokenize_words=True):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences \n",
    "    if do_tokenize_words:\n",
    "        result = [nltk.word_tokenize(sent) for sent in raw_sents]\n",
    "    else: \n",
    "        result = raw_sents\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_simile_candidates(sentences):\n",
    "    comparisons = []\n",
    "    for i_sent, sent in enumerate(sentences):\n",
    "        if not 'like' in sent and not 'as' in sent: \n",
    "            continue \n",
    "        # exlude a single 'as', leaving in only '...as ... as...'\n",
    "        if not 'like' in sent and len([word for word in sent if word=='as']) == 1: \n",
    "            continue\n",
    "        pos_tagged = nltk.pos_tag(sent)\n",
    "        for pair in pos_tagged:\n",
    "            if pair[1] == 'IN' and (pair[0] == 'like' or pair[0] == 'as'):\n",
    "                comparisons.append((i_sent, pos_tagged))\n",
    "    return comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_candidates(all_candidates, regex_filter):\n",
    "    similes_candidates = []\n",
    "    punkt = set(['.',',','-',':',';','!','?', '\"', '\\'', ')', '(', '%', '#', '[', ']', '@'])\n",
    "    key_pos_tags = set(['NN', 'NNS', 'NNP']) #, 'VB', 'VBN', 'VBD', 'VBG']) # noun or verb\n",
    "    for i_sent, tagged_sent in all_candidates:\n",
    "        sent = [pair[0] for pair in tagged_sent]\n",
    "        string = ' '.join(sent)\n",
    "        if regex_filter and re.search(regex_filter, string):\n",
    "            similes_candidates.append((i_sent, string, False))\n",
    "            continue # flat out reject\n",
    "            \n",
    "        start_index = -1\n",
    "        words_after = -1\n",
    "        pos_tags = [pair[1] for pair in tagged_sent]\n",
    "        if 'like' in sent:\n",
    "            start_index = sent.index('like')\n",
    "            #two_words_before_like = max(0, index_of_like - 4)\n",
    "            words_after = min(len(sent), start_index + 6)\n",
    "        elif 'as' in sent:\n",
    "            start_index = sent.index('as')\n",
    "            words_after = min(len(sent), start_index + 8)\n",
    "\n",
    "        if start_index >= 0 and words_after > 0:\n",
    "            index_of_punkt = 0\n",
    "            for i in range(start_index, words_after): \n",
    "                if sent[i] in punkt: \n",
    "                    index_of_punkt = i\n",
    "                    break \n",
    "\n",
    "            if index_of_punkt > start_index: \n",
    "                words_after = min(words_after, index_of_punkt)\n",
    "            subphrase = sent[start_index:words_after]\n",
    "            if not(not key_pos_tags.intersection(set(pos_tags[start_index:words_after]))): # make sure at least one key pos tag is present\n",
    "                similes_candidates.append((i_sent, subphrase, True))\n",
    "            else:\n",
    "                similes_candidates.append((i_sent, subphrase, False))\n",
    "    return similes_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(['a', 'an', 'and', 'or', 'the', \\\n",
    "                  'his', 'her', 'my', 'your', 'their', 'our', \\\n",
    "                  'i', 'you', 'he', 'she', 'it', 'they', 'who', 'that', 'whose', \\\n",
    "                  'is', 'are', 'was', 'will', 'would', \\\n",
    "                  '.',',','-',':',';','!','?', '\"', '\\'', ')', '(', '%', '#', '[', ']', '@'])\n",
    "\n",
    "def preprocess_words(wordlist): \n",
    "    wordset = set([])\n",
    "    for word in wordlist: \n",
    "        word = word.lower()\n",
    "        if word not in stop_words and len(word) > 1: \n",
    "            if word != 'as':\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "            if word == 'like' or word == 'as': \n",
    "                word = '$cmpr'\n",
    "            wordset.add(word)\n",
    "    return wordset \n",
    "        \n",
    "''' Precomputes a corpus (phrase search index) for a given list of phrases\n",
    "    Optimization: create a data structure to speed up fuzzy matching as follows: \n",
    "    {'word' : [i, j, k, ...]}, where i, j, k are the row indices of all phrases containing 'word'. \n",
    "    For each new search query, we prefetch the relevant rows based in the words in that query, \n",
    "    prior to fuzzy matching. \n",
    "'''\n",
    "def init_corpus_2match(wordlists): \n",
    "    lookup = {}\n",
    "    all_wordsets = []\n",
    "    for i_sent, words in wordlists: # for each phrase (word list)\n",
    "        if not words:\n",
    "            continue\n",
    "        wordset = preprocess_words(words)\n",
    "        if not(not wordset):\n",
    "            i_row = len(all_wordsets)   \n",
    "            all_wordsets.append(wordset)\n",
    "            \n",
    "            # update loookup index (dictionary of word to corpus row id)\n",
    "            for word in wordset: \n",
    "                if word not in lookup: \n",
    "                    lookup[word] = [i_row]\n",
    "                else: \n",
    "                    lookup[word].append(i_row)\n",
    "    return (all_wordsets, lookup) \n",
    "\n",
    "\n",
    "''' Returns a list of matches for 'phrase' in 'wordsets' with 'min_similarity' \n",
    "'''\n",
    "def fuzzy_match(words_in, search_index, min_similarity): \n",
    "    # init \n",
    "    phraset = preprocess_words(words_in)\n",
    "    relevant_corpus_rows = search_index\n",
    "    \n",
    "    # prepare relevant subset of search index\n",
    "    # the data could be in 2 different representations\n",
    "    if isinstance(search_index, tuple): \n",
    "        corpus = search_index[0]\n",
    "        lookup = search_index[1]\n",
    "\n",
    "        # prefetch relevant corpus rows \n",
    "        relevant_corpus_row_ids = set([])\n",
    "        for word in phraset: \n",
    "            if word not in lookup or word == '$cmpr':\n",
    "                continue\n",
    "            row_ids = lookup[word]\n",
    "            for i in row_ids:\n",
    "                relevant_corpus_row_ids.add(i)    \n",
    "        relevant_corpus_rows = [corpus[i] for i in relevant_corpus_row_ids]  \n",
    "    \n",
    "    # todo: remove\n",
    "    # print(\"input phrase: {}\".format(phraset))\n",
    "    # print(\"relevant phrase ids: {}\".format(relevant_corpus_row_ids))\n",
    "    \n",
    "    \n",
    "    # actually search\n",
    "    nb_input = len(phraset)\n",
    "    matches = []\n",
    "    for wordset in relevant_corpus_rows: \n",
    "        intersect = phraset.intersection(wordset)\n",
    "        n = len(intersect)\n",
    "        if n/min(nb_input, len(wordset)) >= min_similarity and not(n < 2 and next(iter(intersect))=='$cmpr'): \n",
    "            #print(wordset)\n",
    "            matches.append(wordset)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def train_similes_corpus(candidates):\n",
    "    corpus_2match = init_corpus_2match(candidates)\n",
    "    covered = set([])\n",
    "    count_dict = {}\n",
    "    for cand in candidates:\n",
    "        if not cand: \n",
    "            continue\n",
    "        phrase = ' '.join(cand)\n",
    "        if phrase in covered:\n",
    "            continue\n",
    "        covered.add(phrase)\n",
    "        result = fuzzy_match(cand, corpus_2match, 0.75)\n",
    "        #print(\"result is {}\".format(result))\n",
    "        if result:\n",
    "            count_dict[phrase] = len(result)\n",
    "    \n",
    "    sorted_counts = sorted(count_dict.items(), key=operator.itemgetter(1))\n",
    "    sorted_counts.reverse()\n",
    "    return count_dict, sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def aggregate_similes_candidates(input_dir):  \n",
    "    fList=os.listdir(input_dir)\n",
    "    # Create a list of file names with full path, eliminating subdirectory entries\n",
    "    fList1 = [os.path.join(input_dir, f) for f in fList if os.path.isfile(os.path.join(input_dir, f))] \n",
    "    \n",
    "    #max_files = 1000 #remove to get the entire corpus\n",
    "    all_candidates = []\n",
    "    for i in tqdm(range(len(fList1))): #[0:max_files] \n",
    "        file = fList1[i]\n",
    "        with codecs.open(file, 'r', 'utf-8') as f: \n",
    "                                        # 'latin_1') as f:\n",
    "        #with open(file, encoding=\"utf8\") as f:\n",
    "            raw_text = ''.join(f.read()) \n",
    "            text = re.sub(r\"(\\n|\\r)+\"\"|(@\\w+)+\", ' ', raw_text) #remove backslashes and words starting with @\n",
    "            sentences = tokenize_text(text)\n",
    "            similes_candidates = extract_simile_candidates(sentences)\n",
    "            similes_candidates = filter_candidates(similes_candidates, regex_filter)\n",
    "            all_candidates.extend(similes_candidates)\n",
    "    return all_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract simile candidates from raw text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "def train(input_dir, min_simile_freq): \n",
    "    similes_candidates = aggregate_similes_candidates(input_dir)\n",
    "    count_dict, sorted_counts = train_similes_corpus(similes_candidates)\n",
    "\n",
    "    # create actual corpus and save \n",
    "    top_similes_corpus = init_corpus_2match([(-1, item[0].split(' ')) for item in count_dict.items() if item[1] >= min_simile_freq])\n",
    "    # save \n",
    "    joblib.dump(top_similes_corpus, \"top_similes_corpus.pkl\")\n",
    "    return similes_candidates, sorted_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![](http://www.eventprophire.com/_images/products/large/danger_skull_sign_orange.jpg) Train (don't run it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../raw_data/similes_train_tmp/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-73fd55f561fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimiles_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_simile_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msorted_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-bf45ef736600>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_dir, min_simile_freq)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_simile_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msimiles_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_similes_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcount_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_similes_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimiles_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-41361c61efad>\u001b[0m in \u001b[0;36maggregate_similes_candidates\u001b[0;34m(input_dir)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maggregate_similes_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create a list of file names with full path, eliminating subdirectory entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfList1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfList\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../raw_data/similes_train_tmp/'"
     ]
    }
   ],
   "source": [
    "\n",
    "#similes_candidates, sorted_counts = train(train_dir_name, min_simile_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_tagged_simile_sents(sentences):\n",
    "    simile_sents = []\n",
    "    for sent in sentences: \n",
    "        if not re.search(\"<rule1s>\", sent):\n",
    "            continue\n",
    "        sent = re.sub(r\"(<rule1s>)|(</rule1s>)\", \"\", sent)\n",
    "        simile_sents.append(nltk.pos_tag(nltk.word_tokenize(sent))) \n",
    "    return simile_sents\n",
    "\n",
    "\n",
    "def eval(sentence_text, similes_corpus, min_simile_freq): \n",
    "    sentences = tokenize_text(sentence_text, do_tokenize_words=True)\n",
    "    sentences_orig = tokenize_text(sentence_text, do_tokenize_words=False)\n",
    "        \n",
    "    similes_candidates = extract_simile_candidates(sentences)\n",
    "    similes_candidates = filter_candidates(similes_candidates, regex_filter)\n",
    "    results = []\n",
    "    for i_sent, cand, is_pred_simile in similes_candidates:\n",
    "        if is_pred_simile:\n",
    "            matches = fuzzy_match(cand, similes_corpus, 0.75)\n",
    "            nb_matches = len(matches)\n",
    "            if nb_matches >= min_simile_freq:\n",
    "                is_pred_simile = True\n",
    "            else:\n",
    "                is_pred_simile = False\n",
    "                 \n",
    "        # LIKE vs. AS\n",
    "        sub_index = 0\n",
    "        sub_length = 0\n",
    "        if 'like' in cand:\n",
    "            sub_index = sentences_orig[i_sent].find('like')\n",
    "            sub_length = 4\n",
    "        elif 'as' in cand:\n",
    "            sub_index = sentences_orig[i_sent].find('as')\n",
    "            sub_length = 2\n",
    "        global_index = sentence_text.find(sentences_orig[i_sent]) \n",
    "        results.append((is_pred_simile, # is simile? \n",
    "                        i_sent, # sentence index\n",
    "                        global_index+sub_index, # index of first char of the sentence in the full text \n",
    "                        sub_length, # comparison string length\n",
    "                        sentences_orig[i_sent], cand)) # simile\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Test last step: (pseudo-)\"classification\" of simile_candidates\n",
    "def test(data_dir, similes_corpus, min_simile_freq): \n",
    "    raw_corpus = get_raw_text_data(data_dir)\n",
    "    sentences = tokenize_text(raw_corpus, do_tokenize_words=False)\n",
    "    true_simile_sents = extract_tagged_simile_sents(sentences)\n",
    "    \n",
    "    nb_true_pos = 0\n",
    "    false_pos = []\n",
    "    false_neg = []\n",
    "    for true_simile_sent in true_simile_sents:\n",
    "        # sent_words = [pair[0] for pair in tagged_sent]\n",
    "        is_pred_simile = False\n",
    "        sent = [pair[0] for pair in true_simile_sent] # remove POS tags \n",
    "        nb_matches = 0\n",
    "        nb_true_pos += 1\n",
    "        simile_candidates = filter_candidates([true_simile_sent], regex_filter)\n",
    "        if not (not simile_candidates): \n",
    "            simile_candidate = simile_candidates[0]\n",
    "            matches = fuzzy_match(simile_candidate[1], similes_corpus, 0.75)\n",
    "            nb_matches = len(matches)\n",
    "        \n",
    "        if nb_matches >= min_simile_freq:\n",
    "            is_pred_simile = True\n",
    "\n",
    "        if not is_pred_simile:\n",
    "            false_neg.append(' '.join(sent))\n",
    "#         else \n",
    "#             print(\"'{}' is NOT a trite simile\".format(cand))\n",
    "#    precision = nb_true_pos / (nb_true_pos + len(false_pos))\n",
    "#    recall = nb_true_pos / (nb_true_pos + len(false_neg))\n",
    "#     print(\"=== Claddification Report ===\")\n",
    "#     print(\"Precision = {}\".format(precision))\n",
    "#     print(\"Recall = {}\".format(recall))\n",
    "#     print(\"=============================\")\n",
    "#     print (\"-- False Negatives --\")\n",
    "#     for neg in false_neg:\n",
    "#         print(neg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 153.84it/s]\n"
     ]
    }
   ],
   "source": [
    "similes_corpus = joblib.load(\"top_similes_corpus.pkl\")\n",
    "test(test_dir_name, similes_corpus, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True,\n",
       "  0,\n",
       "  35,\n",
       "  4,\n",
       "  'Mysterious Mr. Fogg lives his life like a machine, really.',\n",
       "  ['like', 'a', 'machine']),\n",
       " (False,\n",
       "  1,\n",
       "  77,\n",
       "  4,\n",
       "  'In fact, he looks like a frog.',\n",
       "  'In fact , he looks like a frog .'),\n",
       " (False,\n",
       "  3,\n",
       "  121,\n",
       "  2,\n",
       "  'He is as green as a frog.',\n",
       "  ['as', 'green', 'as', 'a', 'frog'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Misc unit tests \n",
    "#test_data = \"Mysterious Mr. Fogg lives his life like a machine, really. In fact, he looks like a frog. That's the honest truth. He is as green as a frog.\"\n",
    "#eval(test_data, similes_corpus, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trite_similes(document):\n",
    "    trite_similes = []\n",
    "    raw_return = eval(document, similes_corpus, 20)\n",
    "    for each in raw_return:\n",
    "        if each[0] is True:\n",
    "            trite_similes.append((each[2], each[3]))\n",
    "    return trite_similes\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nontrite_similes(document):\n",
    "    nontrite_similes = []\n",
    "    raw_return = eval(document, similes_corpus, 20)\n",
    "    for each in raw_return:\n",
    "        if each[0] is False:\n",
    "            nontrite_similes.append((each[2], each[3]))\n",
    "    return nontrite_similes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(35, 4)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trite_similes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(77, 4), (121, 2)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nontrite_similes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
